2025-10-27 01:58:06,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 01:58:06,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 01:58:06,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 01:58:06,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 01:58:06,554:INFO:PyCaret ClassificationExperiment
2025-10-27 01:58:06,554:INFO:Logging name: clf-default-name
2025-10-27 01:58:06,554:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-27 01:58:06,554:INFO:version 3.3.2
2025-10-27 01:58:06,554:INFO:Initializing setup()
2025-10-27 01:58:06,554:INFO:self.USI: ee62
2025-10-27 01:58:06,554:INFO:self._variable_keys: {'is_multiclass', 'memory', 'seed', 'pipeline', 'exp_id', 'fold_generator', 'fold_shuffle_param', 'y', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'gpu_param', '_available_plots', 'exp_name_log', 'n_jobs_param', 'USI', 'html_param', 'log_plots_param', 'logging_param', 'target_param', 'X_test', 'data', 'y_test', 'fold_groups_param', 'idx', 'y_train', 'X', 'X_train'}
2025-10-27 01:58:06,554:INFO:Checking environment
2025-10-27 01:58:06,554:INFO:python_version: 3.11.14
2025-10-27 01:58:06,554:INFO:python_build: ('main', 'Oct 24 2025 09:18:30')
2025-10-27 01:58:06,554:INFO:machine: x86_64
2025-10-27 01:58:06,554:INFO:platform: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 01:58:06,554:INFO:Memory: svmem(total=16365076480, available=8902324224, percent=45.6, used=7462752256, free=2061963264, active=4672962560, inactive=6830579712, buffers=2609152, cached=8663830528, shared=1377554432, slab=1275830272)
2025-10-27 01:58:06,555:INFO:Physical Core: 12
2025-10-27 01:58:06,555:INFO:Logical Core: 16
2025-10-27 01:58:06,555:INFO:Checking libraries
2025-10-27 01:58:06,555:INFO:System:
2025-10-27 01:58:06,555:INFO:    python: 3.11.14 (main, Oct 24 2025, 09:18:30) [GCC 15.2.1 20250813]
2025-10-27 01:58:06,555:INFO:executable: /home/ykalathiya/pycaret_assignment/.venv/bin/python
2025-10-27 01:58:06,555:INFO:   machine: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 01:58:06,555:INFO:PyCaret required dependencies:
2025-10-27 01:58:06,587:INFO:                 pip: 25.2
2025-10-27 01:58:06,587:INFO:          setuptools: 80.9.0
2025-10-27 01:58:06,587:INFO:             pycaret: 3.3.2
2025-10-27 01:58:06,587:INFO:             IPython: 9.6.0
2025-10-27 01:58:06,587:INFO:          ipywidgets: 8.1.7
2025-10-27 01:58:06,587:INFO:                tqdm: 4.67.1
2025-10-27 01:58:06,587:INFO:               numpy: 1.26.4
2025-10-27 01:58:06,587:INFO:              pandas: 2.1.4
2025-10-27 01:58:06,587:INFO:              jinja2: 3.1.6
2025-10-27 01:58:06,587:INFO:               scipy: 1.11.4
2025-10-27 01:58:06,587:INFO:              joblib: 1.3.2
2025-10-27 01:58:06,587:INFO:             sklearn: 1.4.2
2025-10-27 01:58:06,587:INFO:                pyod: 2.0.5
2025-10-27 01:58:06,587:INFO:            imblearn: 0.14.0
2025-10-27 01:58:06,587:INFO:   category_encoders: 2.7.0
2025-10-27 01:58:06,587:INFO:            lightgbm: 4.6.0
2025-10-27 01:58:06,587:INFO:               numba: 0.61.0
2025-10-27 01:58:06,587:INFO:            requests: 2.32.5
2025-10-27 01:58:06,587:INFO:          matplotlib: 3.7.5
2025-10-27 01:58:06,587:INFO:          scikitplot: 0.3.7
2025-10-27 01:58:06,587:INFO:         yellowbrick: 1.5
2025-10-27 01:58:06,587:INFO:              plotly: 5.24.1
2025-10-27 01:58:06,587:INFO:    plotly-resampler: Not installed
2025-10-27 01:58:06,587:INFO:             kaleido: 1.1.0
2025-10-27 01:58:06,587:INFO:           schemdraw: 0.15
2025-10-27 01:58:06,587:INFO:         statsmodels: 0.14.5
2025-10-27 01:58:06,587:INFO:              sktime: 0.26.0
2025-10-27 01:58:06,587:INFO:               tbats: 1.1.3
2025-10-27 01:58:06,587:INFO:            pmdarima: 2.0.4
2025-10-27 01:58:06,587:INFO:              psutil: 7.1.1
2025-10-27 01:58:06,587:INFO:          markupsafe: 3.0.3
2025-10-27 01:58:06,587:INFO:             pickle5: Not installed
2025-10-27 01:58:06,587:INFO:         cloudpickle: 3.1.1
2025-10-27 01:58:06,587:INFO:         deprecation: 2.1.0
2025-10-27 01:58:06,587:INFO:              xxhash: 3.6.0
2025-10-27 01:58:06,587:INFO:           wurlitzer: 3.1.1
2025-10-27 01:58:06,587:INFO:PyCaret optional dependencies:
2025-10-27 01:58:07,107:INFO:                shap: 0.44.1
2025-10-27 01:58:07,107:INFO:           interpret: 0.7.3
2025-10-27 01:58:07,107:INFO:                umap: 0.5.7
2025-10-27 01:58:07,107:INFO:     ydata_profiling: 4.17.0
2025-10-27 01:58:07,107:INFO:  explainerdashboard: 0.5.1
2025-10-27 01:58:07,107:INFO:             autoviz: Not installed
2025-10-27 01:58:07,107:INFO:           fairlearn: 0.7.0
2025-10-27 01:58:07,107:INFO:          deepchecks: Not installed
2025-10-27 01:58:07,107:INFO:             xgboost: Not installed
2025-10-27 01:58:07,107:INFO:            catboost: Not installed
2025-10-27 01:58:07,108:INFO:              kmodes: Not installed
2025-10-27 01:58:07,108:INFO:             mlxtend: Not installed
2025-10-27 01:58:07,108:INFO:       statsforecast: Not installed
2025-10-27 01:58:07,108:INFO:        tune_sklearn: Not installed
2025-10-27 01:58:07,108:INFO:                 ray: Not installed
2025-10-27 01:58:07,108:INFO:            hyperopt: Not installed
2025-10-27 01:58:07,108:INFO:              optuna: Not installed
2025-10-27 01:58:07,108:INFO:               skopt: Not installed
2025-10-27 01:58:07,108:INFO:              mlflow: Not installed
2025-10-27 01:58:07,108:INFO:              gradio: Not installed
2025-10-27 01:58:07,108:INFO:             fastapi: Not installed
2025-10-27 01:58:07,108:INFO:             uvicorn: Not installed
2025-10-27 01:58:07,108:INFO:              m2cgen: Not installed
2025-10-27 01:58:07,108:INFO:           evidently: Not installed
2025-10-27 01:58:07,108:INFO:               fugue: Not installed
2025-10-27 01:58:07,108:INFO:           streamlit: Not installed
2025-10-27 01:58:07,108:INFO:             prophet: Not installed
2025-10-27 01:58:07,108:INFO:None
2025-10-27 01:58:07,108:INFO:Set up data.
2025-10-27 01:58:07,111:INFO:Set up folding strategy.
2025-10-27 01:58:07,111:INFO:Set up train/test split.
2025-10-27 01:58:07,114:INFO:Set up index.
2025-10-27 01:58:07,114:INFO:Assigning column types.
2025-10-27 01:58:07,116:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-27 01:58:07,137:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,139:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,156:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,156:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,177:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,177:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,191:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,191:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,191:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-27 01:58:07,213:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,227:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,249:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 01:58:07,262:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,263:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-27 01:58:07,298:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,335:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,335:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,335:INFO:Preparing preprocessing pipeline...
2025-10-27 01:58:07,336:INFO:Set up simple imputation.
2025-10-27 01:58:07,336:INFO:Set up column transformation.
2025-10-27 01:58:07,336:INFO:Set up feature normalization.
2025-10-27 01:58:07,370:INFO:Finished creating preprocessing pipeline.
2025-10-27 01:58:07,375:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-27 01:58:07,375:INFO:Creating final display dataframe.
2025-10-27 01:58:07,425:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 15)
5   Transformed train set shape        (1674, 15)
6    Transformed test set shape         (718, 15)
7              Numeric features                14
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              ee62
2025-10-27 01:58:07,468:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,468:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,509:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,509:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 01:58:07,510:INFO:setup() successfully completed in 0.96s...............
2025-10-27 01:58:07,521:INFO:Initializing compare_models()
2025-10-27 01:58:07,521:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-10-27 01:58:07,521:INFO:Checking exceptions
2025-10-27 01:58:07,525:INFO:Preparing display monitor
2025-10-27 01:58:07,538:INFO:Initializing Logistic Regression
2025-10-27 01:58:07,538:INFO:Total runtime is 2.4437904357910156e-06 minutes
2025-10-27 01:58:07,540:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:07,540:INFO:Initializing create_model()
2025-10-27 01:58:07,540:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:07,540:INFO:Checking exceptions
2025-10-27 01:58:07,540:INFO:Importing libraries
2025-10-27 01:58:07,540:INFO:Copying training dataset
2025-10-27 01:58:07,543:INFO:Defining folds
2025-10-27 01:58:07,543:INFO:Declaring metric variables
2025-10-27 01:58:07,546:INFO:Importing untrained model
2025-10-27 01:58:07,548:INFO:Logistic Regression Imported successfully
2025-10-27 01:58:07,551:INFO:Starting cross validation
2025-10-27 01:58:07,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:09,105:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,163:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,200:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,310:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,381:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,388:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,466:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,471:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:09,490:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,519:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,523:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:09,538:INFO:Calculating mean and std
2025-10-27 01:58:09,540:INFO:Creating metrics dataframe
2025-10-27 01:58:09,543:INFO:Uploading results into container
2025-10-27 01:58:09,543:INFO:Uploading model into container now
2025-10-27 01:58:09,544:INFO:_master_model_container: 1
2025-10-27 01:58:09,544:INFO:_display_container: 2
2025-10-27 01:58:09,544:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-27 01:58:09,544:INFO:create_model() successfully completed......................................
2025-10-27 01:58:09,660:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:09,660:INFO:Creating metrics dataframe
2025-10-27 01:58:09,664:INFO:Initializing K Neighbors Classifier
2025-10-27 01:58:09,664:INFO:Total runtime is 0.03544740279515584 minutes
2025-10-27 01:58:09,666:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:09,666:INFO:Initializing create_model()
2025-10-27 01:58:09,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:09,667:INFO:Checking exceptions
2025-10-27 01:58:09,667:INFO:Importing libraries
2025-10-27 01:58:09,667:INFO:Copying training dataset
2025-10-27 01:58:09,672:INFO:Defining folds
2025-10-27 01:58:09,673:INFO:Declaring metric variables
2025-10-27 01:58:09,675:INFO:Importing untrained model
2025-10-27 01:58:09,677:INFO:K Neighbors Classifier Imported successfully
2025-10-27 01:58:09,681:INFO:Starting cross validation
2025-10-27 01:58:09,682:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:09,777:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,083:INFO:Calculating mean and std
2025-10-27 01:58:11,085:INFO:Creating metrics dataframe
2025-10-27 01:58:11,087:INFO:Uploading results into container
2025-10-27 01:58:11,087:INFO:Uploading model into container now
2025-10-27 01:58:11,087:INFO:_master_model_container: 2
2025-10-27 01:58:11,087:INFO:_display_container: 2
2025-10-27 01:58:11,088:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-27 01:58:11,088:INFO:create_model() successfully completed......................................
2025-10-27 01:58:11,173:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:11,173:INFO:Creating metrics dataframe
2025-10-27 01:58:11,177:INFO:Initializing Naive Bayes
2025-10-27 01:58:11,177:INFO:Total runtime is 0.06066062450408935 minutes
2025-10-27 01:58:11,179:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:11,179:INFO:Initializing create_model()
2025-10-27 01:58:11,179:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:11,179:INFO:Checking exceptions
2025-10-27 01:58:11,179:INFO:Importing libraries
2025-10-27 01:58:11,179:INFO:Copying training dataset
2025-10-27 01:58:11,181:INFO:Defining folds
2025-10-27 01:58:11,181:INFO:Declaring metric variables
2025-10-27 01:58:11,182:INFO:Importing untrained model
2025-10-27 01:58:11,184:INFO:Naive Bayes Imported successfully
2025-10-27 01:58:11,186:INFO:Starting cross validation
2025-10-27 01:58:11,187:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:11,241:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,253:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,263:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,265:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,268:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,275:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,285:INFO:Calculating mean and std
2025-10-27 01:58:11,286:INFO:Creating metrics dataframe
2025-10-27 01:58:11,287:INFO:Uploading results into container
2025-10-27 01:58:11,287:INFO:Uploading model into container now
2025-10-27 01:58:11,288:INFO:_master_model_container: 3
2025-10-27 01:58:11,288:INFO:_display_container: 2
2025-10-27 01:58:11,288:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-27 01:58:11,288:INFO:create_model() successfully completed......................................
2025-10-27 01:58:11,369:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:11,369:INFO:Creating metrics dataframe
2025-10-27 01:58:11,373:INFO:Initializing Decision Tree Classifier
2025-10-27 01:58:11,373:INFO:Total runtime is 0.06392718950907389 minutes
2025-10-27 01:58:11,375:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:11,375:INFO:Initializing create_model()
2025-10-27 01:58:11,375:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:11,375:INFO:Checking exceptions
2025-10-27 01:58:11,375:INFO:Importing libraries
2025-10-27 01:58:11,375:INFO:Copying training dataset
2025-10-27 01:58:11,377:INFO:Defining folds
2025-10-27 01:58:11,377:INFO:Declaring metric variables
2025-10-27 01:58:11,379:INFO:Importing untrained model
2025-10-27 01:58:11,380:INFO:Decision Tree Classifier Imported successfully
2025-10-27 01:58:11,383:INFO:Starting cross validation
2025-10-27 01:58:11,383:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:11,501:INFO:Calculating mean and std
2025-10-27 01:58:11,503:INFO:Creating metrics dataframe
2025-10-27 01:58:11,504:INFO:Uploading results into container
2025-10-27 01:58:11,504:INFO:Uploading model into container now
2025-10-27 01:58:11,505:INFO:_master_model_container: 4
2025-10-27 01:58:11,505:INFO:_display_container: 2
2025-10-27 01:58:11,505:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-10-27 01:58:11,505:INFO:create_model() successfully completed......................................
2025-10-27 01:58:11,590:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:11,590:INFO:Creating metrics dataframe
2025-10-27 01:58:11,594:INFO:Initializing SVM - Linear Kernel
2025-10-27 01:58:11,594:INFO:Total runtime is 0.06760646104812622 minutes
2025-10-27 01:58:11,596:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:11,596:INFO:Initializing create_model()
2025-10-27 01:58:11,596:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:11,596:INFO:Checking exceptions
2025-10-27 01:58:11,596:INFO:Importing libraries
2025-10-27 01:58:11,596:INFO:Copying training dataset
2025-10-27 01:58:11,598:INFO:Defining folds
2025-10-27 01:58:11,598:INFO:Declaring metric variables
2025-10-27 01:58:11,600:INFO:Importing untrained model
2025-10-27 01:58:11,601:INFO:SVM - Linear Kernel Imported successfully
2025-10-27 01:58:11,606:INFO:Starting cross validation
2025-10-27 01:58:11,607:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:11,675:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,676:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,679:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,685:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,687:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,691:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,695:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,708:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,712:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,722:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,725:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,725:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,736:INFO:Calculating mean and std
2025-10-27 01:58:11,736:INFO:Creating metrics dataframe
2025-10-27 01:58:11,738:INFO:Uploading results into container
2025-10-27 01:58:11,738:INFO:Uploading model into container now
2025-10-27 01:58:11,738:INFO:_master_model_container: 5
2025-10-27 01:58:11,738:INFO:_display_container: 2
2025-10-27 01:58:11,738:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-27 01:58:11,739:INFO:create_model() successfully completed......................................
2025-10-27 01:58:11,817:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:11,817:INFO:Creating metrics dataframe
2025-10-27 01:58:11,821:INFO:Initializing Ridge Classifier
2025-10-27 01:58:11,822:INFO:Total runtime is 0.07139782508214315 minutes
2025-10-27 01:58:11,823:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:11,823:INFO:Initializing create_model()
2025-10-27 01:58:11,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:11,824:INFO:Checking exceptions
2025-10-27 01:58:11,824:INFO:Importing libraries
2025-10-27 01:58:11,824:INFO:Copying training dataset
2025-10-27 01:58:11,826:INFO:Defining folds
2025-10-27 01:58:11,826:INFO:Declaring metric variables
2025-10-27 01:58:11,827:INFO:Importing untrained model
2025-10-27 01:58:11,829:INFO:Ridge Classifier Imported successfully
2025-10-27 01:58:11,832:INFO:Starting cross validation
2025-10-27 01:58:11,832:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:11,868:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,870:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,872:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,872:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,874:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,874:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,875:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,877:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,893:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,894:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,895:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,896:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,896:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,899:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,907:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,908:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,909:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,910:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,915:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:11,917:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:11,929:INFO:Calculating mean and std
2025-10-27 01:58:11,930:INFO:Creating metrics dataframe
2025-10-27 01:58:11,932:INFO:Uploading results into container
2025-10-27 01:58:11,932:INFO:Uploading model into container now
2025-10-27 01:58:11,932:INFO:_master_model_container: 6
2025-10-27 01:58:11,932:INFO:_display_container: 2
2025-10-27 01:58:11,932:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-10-27 01:58:11,932:INFO:create_model() successfully completed......................................
2025-10-27 01:58:12,016:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:12,016:INFO:Creating metrics dataframe
2025-10-27 01:58:12,020:INFO:Initializing Random Forest Classifier
2025-10-27 01:58:12,020:INFO:Total runtime is 0.07470708290735881 minutes
2025-10-27 01:58:12,022:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:12,022:INFO:Initializing create_model()
2025-10-27 01:58:12,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:12,022:INFO:Checking exceptions
2025-10-27 01:58:12,022:INFO:Importing libraries
2025-10-27 01:58:12,022:INFO:Copying training dataset
2025-10-27 01:58:12,024:INFO:Defining folds
2025-10-27 01:58:12,024:INFO:Declaring metric variables
2025-10-27 01:58:12,026:INFO:Importing untrained model
2025-10-27 01:58:12,027:INFO:Random Forest Classifier Imported successfully
2025-10-27 01:58:12,029:INFO:Starting cross validation
2025-10-27 01:58:12,030:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:12,400:INFO:Calculating mean and std
2025-10-27 01:58:12,401:INFO:Creating metrics dataframe
2025-10-27 01:58:12,403:INFO:Uploading results into container
2025-10-27 01:58:12,403:INFO:Uploading model into container now
2025-10-27 01:58:12,403:INFO:_master_model_container: 7
2025-10-27 01:58:12,403:INFO:_display_container: 2
2025-10-27 01:58:12,404:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-10-27 01:58:12,404:INFO:create_model() successfully completed......................................
2025-10-27 01:58:12,486:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:12,486:INFO:Creating metrics dataframe
2025-10-27 01:58:12,491:INFO:Initializing Quadratic Discriminant Analysis
2025-10-27 01:58:12,491:INFO:Total runtime is 0.08255178928375244 minutes
2025-10-27 01:58:12,492:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:12,492:INFO:Initializing create_model()
2025-10-27 01:58:12,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:12,492:INFO:Checking exceptions
2025-10-27 01:58:12,492:INFO:Importing libraries
2025-10-27 01:58:12,493:INFO:Copying training dataset
2025-10-27 01:58:12,495:INFO:Defining folds
2025-10-27 01:58:12,495:INFO:Declaring metric variables
2025-10-27 01:58:12,496:INFO:Importing untrained model
2025-10-27 01:58:12,497:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-27 01:58:12,500:INFO:Starting cross validation
2025-10-27 01:58:12,500:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:12,555:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,557:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,558:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,562:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,563:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,570:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,576:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,577:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,579:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,580:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,587:INFO:Calculating mean and std
2025-10-27 01:58:12,588:INFO:Creating metrics dataframe
2025-10-27 01:58:12,589:INFO:Uploading results into container
2025-10-27 01:58:12,590:INFO:Uploading model into container now
2025-10-27 01:58:12,590:INFO:_master_model_container: 8
2025-10-27 01:58:12,590:INFO:_display_container: 2
2025-10-27 01:58:12,590:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-27 01:58:12,590:INFO:create_model() successfully completed......................................
2025-10-27 01:58:12,669:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:12,669:INFO:Creating metrics dataframe
2025-10-27 01:58:12,674:INFO:Initializing Ada Boost Classifier
2025-10-27 01:58:12,674:INFO:Total runtime is 0.08560335238774618 minutes
2025-10-27 01:58:12,675:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:12,676:INFO:Initializing create_model()
2025-10-27 01:58:12,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:12,676:INFO:Checking exceptions
2025-10-27 01:58:12,676:INFO:Importing libraries
2025-10-27 01:58:12,676:INFO:Copying training dataset
2025-10-27 01:58:12,678:INFO:Defining folds
2025-10-27 01:58:12,678:INFO:Declaring metric variables
2025-10-27 01:58:12,679:INFO:Importing untrained model
2025-10-27 01:58:12,681:INFO:Ada Boost Classifier Imported successfully
2025-10-27 01:58:12,685:INFO:Starting cross validation
2025-10-27 01:58:12,685:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:12,722:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,723:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,735:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,736:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,737:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,740:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,763:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,764:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,766:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,770:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 01:58:12,831:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,842:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,844:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,846:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,873:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,896:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,899:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,907:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,918:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,924:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:12,937:INFO:Calculating mean and std
2025-10-27 01:58:12,938:INFO:Creating metrics dataframe
2025-10-27 01:58:12,939:INFO:Uploading results into container
2025-10-27 01:58:12,939:INFO:Uploading model into container now
2025-10-27 01:58:12,940:INFO:_master_model_container: 9
2025-10-27 01:58:12,940:INFO:_display_container: 2
2025-10-27 01:58:12,940:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-10-27 01:58:12,940:INFO:create_model() successfully completed......................................
2025-10-27 01:58:13,018:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:13,018:INFO:Creating metrics dataframe
2025-10-27 01:58:13,023:INFO:Initializing Gradient Boosting Classifier
2025-10-27 01:58:13,023:INFO:Total runtime is 0.09141689538955688 minutes
2025-10-27 01:58:13,024:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:13,024:INFO:Initializing create_model()
2025-10-27 01:58:13,024:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:13,025:INFO:Checking exceptions
2025-10-27 01:58:13,025:INFO:Importing libraries
2025-10-27 01:58:13,025:INFO:Copying training dataset
2025-10-27 01:58:13,027:INFO:Defining folds
2025-10-27 01:58:13,027:INFO:Declaring metric variables
2025-10-27 01:58:13,028:INFO:Importing untrained model
2025-10-27 01:58:13,029:INFO:Gradient Boosting Classifier Imported successfully
2025-10-27 01:58:13,032:INFO:Starting cross validation
2025-10-27 01:58:13,032:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:14,324:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,339:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,410:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,497:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,585:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,774:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,796:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,832:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,884:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,925:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:14,936:INFO:Calculating mean and std
2025-10-27 01:58:14,938:INFO:Creating metrics dataframe
2025-10-27 01:58:14,940:INFO:Uploading results into container
2025-10-27 01:58:14,941:INFO:Uploading model into container now
2025-10-27 01:58:14,941:INFO:_master_model_container: 10
2025-10-27 01:58:14,941:INFO:_display_container: 2
2025-10-27 01:58:14,942:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-27 01:58:14,942:INFO:create_model() successfully completed......................................
2025-10-27 01:58:15,035:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:15,035:INFO:Creating metrics dataframe
2025-10-27 01:58:15,040:INFO:Initializing Linear Discriminant Analysis
2025-10-27 01:58:15,040:INFO:Total runtime is 0.1250336527824402 minutes
2025-10-27 01:58:15,042:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:15,042:INFO:Initializing create_model()
2025-10-27 01:58:15,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:15,042:INFO:Checking exceptions
2025-10-27 01:58:15,042:INFO:Importing libraries
2025-10-27 01:58:15,042:INFO:Copying training dataset
2025-10-27 01:58:15,044:INFO:Defining folds
2025-10-27 01:58:15,044:INFO:Declaring metric variables
2025-10-27 01:58:15,047:INFO:Importing untrained model
2025-10-27 01:58:15,049:INFO:Linear Discriminant Analysis Imported successfully
2025-10-27 01:58:15,054:INFO:Starting cross validation
2025-10-27 01:58:15,055:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:15,110:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,110:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,111:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,112:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,113:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:15,119:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,124:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:15,130:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,131:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,139:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,141:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,142:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 01:58:15,153:INFO:Calculating mean and std
2025-10-27 01:58:15,154:INFO:Creating metrics dataframe
2025-10-27 01:58:15,155:INFO:Uploading results into container
2025-10-27 01:58:15,155:INFO:Uploading model into container now
2025-10-27 01:58:15,156:INFO:_master_model_container: 11
2025-10-27 01:58:15,156:INFO:_display_container: 2
2025-10-27 01:58:15,156:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-27 01:58:15,156:INFO:create_model() successfully completed......................................
2025-10-27 01:58:15,239:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:15,239:INFO:Creating metrics dataframe
2025-10-27 01:58:15,244:INFO:Initializing Extra Trees Classifier
2025-10-27 01:58:15,244:INFO:Total runtime is 0.12843695084253948 minutes
2025-10-27 01:58:15,245:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:15,246:INFO:Initializing create_model()
2025-10-27 01:58:15,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:15,246:INFO:Checking exceptions
2025-10-27 01:58:15,246:INFO:Importing libraries
2025-10-27 01:58:15,246:INFO:Copying training dataset
2025-10-27 01:58:15,248:INFO:Defining folds
2025-10-27 01:58:15,248:INFO:Declaring metric variables
2025-10-27 01:58:15,249:INFO:Importing untrained model
2025-10-27 01:58:15,251:INFO:Extra Trees Classifier Imported successfully
2025-10-27 01:58:15,254:INFO:Starting cross validation
2025-10-27 01:58:15,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:15,594:INFO:Calculating mean and std
2025-10-27 01:58:15,595:INFO:Creating metrics dataframe
2025-10-27 01:58:15,596:INFO:Uploading results into container
2025-10-27 01:58:15,596:INFO:Uploading model into container now
2025-10-27 01:58:15,596:INFO:_master_model_container: 12
2025-10-27 01:58:15,596:INFO:_display_container: 2
2025-10-27 01:58:15,597:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-10-27 01:58:15,597:INFO:create_model() successfully completed......................................
2025-10-27 01:58:15,676:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:15,676:INFO:Creating metrics dataframe
2025-10-27 01:58:15,681:INFO:Initializing Light Gradient Boosting Machine
2025-10-27 01:58:15,681:INFO:Total runtime is 0.1357158064842224 minutes
2025-10-27 01:58:15,682:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:15,682:INFO:Initializing create_model()
2025-10-27 01:58:15,682:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:15,682:INFO:Checking exceptions
2025-10-27 01:58:15,682:INFO:Importing libraries
2025-10-27 01:58:15,682:INFO:Copying training dataset
2025-10-27 01:58:15,685:INFO:Defining folds
2025-10-27 01:58:15,685:INFO:Declaring metric variables
2025-10-27 01:58:15,686:INFO:Importing untrained model
2025-10-27 01:58:15,688:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 01:58:15,691:INFO:Starting cross validation
2025-10-27 01:58:15,691:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:27,136:INFO:Calculating mean and std
2025-10-27 01:58:27,137:INFO:Creating metrics dataframe
2025-10-27 01:58:27,138:INFO:Uploading results into container
2025-10-27 01:58:27,139:INFO:Uploading model into container now
2025-10-27 01:58:27,139:INFO:_master_model_container: 13
2025-10-27 01:58:27,140:INFO:_display_container: 2
2025-10-27 01:58:27,140:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 01:58:27,140:INFO:create_model() successfully completed......................................
2025-10-27 01:58:27,217:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:27,217:INFO:Creating metrics dataframe
2025-10-27 01:58:27,222:INFO:Initializing Dummy Classifier
2025-10-27 01:58:27,222:INFO:Total runtime is 0.3280777255694072 minutes
2025-10-27 01:58:27,224:INFO:SubProcess create_model() called ==================================
2025-10-27 01:58:27,224:INFO:Initializing create_model()
2025-10-27 01:58:27,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fadf9236990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:27,224:INFO:Checking exceptions
2025-10-27 01:58:27,224:INFO:Importing libraries
2025-10-27 01:58:27,224:INFO:Copying training dataset
2025-10-27 01:58:27,226:INFO:Defining folds
2025-10-27 01:58:27,226:INFO:Declaring metric variables
2025-10-27 01:58:27,228:INFO:Importing untrained model
2025-10-27 01:58:27,229:INFO:Dummy Classifier Imported successfully
2025-10-27 01:58:27,232:INFO:Starting cross validation
2025-10-27 01:58:27,233:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 01:58:27,285:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,286:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,289:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,291:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,294:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,308:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,311:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,318:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,319:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,323:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 01:58:27,331:INFO:Calculating mean and std
2025-10-27 01:58:27,331:INFO:Creating metrics dataframe
2025-10-27 01:58:27,332:INFO:Uploading results into container
2025-10-27 01:58:27,333:INFO:Uploading model into container now
2025-10-27 01:58:27,333:INFO:_master_model_container: 14
2025-10-27 01:58:27,333:INFO:_display_container: 2
2025-10-27 01:58:27,333:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-10-27 01:58:27,333:INFO:create_model() successfully completed......................................
2025-10-27 01:58:27,411:INFO:SubProcess create_model() end ==================================
2025-10-27 01:58:27,411:INFO:Creating metrics dataframe
2025-10-27 01:58:27,416:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2025-10-27 01:58:27,421:INFO:Initializing create_model()
2025-10-27 01:58:27,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 01:58:27,421:INFO:Checking exceptions
2025-10-27 01:58:27,422:INFO:Importing libraries
2025-10-27 01:58:27,422:INFO:Copying training dataset
2025-10-27 01:58:27,424:INFO:Defining folds
2025-10-27 01:58:27,424:INFO:Declaring metric variables
2025-10-27 01:58:27,424:INFO:Importing untrained model
2025-10-27 01:58:27,424:INFO:Declaring custom model
2025-10-27 01:58:27,425:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 01:58:27,425:INFO:Cross validation set to False
2025-10-27 01:58:27,425:INFO:Fitting Model
2025-10-27 01:58:27,459:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000361 seconds.
2025-10-27 01:58:27,459:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 01:58:27,459:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 01:58:27,459:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 01:58:27,459:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 01:58:27,459:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 01:58:27,459:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 01:58:27,460:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 01:58:27,460:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 01:58:27,460:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 01:58:27,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:27,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 01:58:28,626:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 01:58:28,626:INFO:create_model() successfully completed......................................
2025-10-27 01:58:28,716:INFO:_master_model_container: 14
2025-10-27 01:58:28,716:INFO:_display_container: 2
2025-10-27 01:58:28,716:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 01:58:28,716:INFO:compare_models() successfully completed......................................
2025-10-27 01:58:28,726:INFO:Initializing evaluate_model()
2025-10-27 01:58:28,726:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-10-27 01:58:28,731:INFO:Initializing plot_model()
2025-10-27 01:58:28,731:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-10-27 01:58:28,731:INFO:Checking exceptions
2025-10-27 01:58:28,733:INFO:Preloading libraries
2025-10-27 01:58:28,744:INFO:Copying training dataset
2025-10-27 01:58:28,744:INFO:Plot type: pipeline
2025-10-27 01:58:28,884:INFO:Visual Rendered Successfully
2025-10-27 01:58:28,968:INFO:plot_model() successfully completed......................................
2025-10-27 01:58:28,981:INFO:Initializing save_model()
2025-10-27 01:58:28,982:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=best_student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-10-27 01:58:28,982:INFO:Adding model into prep_pipe
2025-10-27 01:58:28,997:INFO:best_student_performance_model.pkl saved in current working directory
2025-10-27 01:58:29,001:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=No...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-10-27 01:58:29,001:INFO:save_model() successfully completed......................................
2025-10-27 01:58:29,098:INFO:Initializing predict_model()
2025-10-27 01:58:29,098:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae02199fd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fadf8f11f80>)
2025-10-27 01:58:29,098:INFO:Checking exceptions
2025-10-27 01:58:29,098:INFO:Preloading libraries
2025-10-27 02:00:46,494:INFO:PyCaret ClassificationExperiment
2025-10-27 02:00:46,494:INFO:Logging name: clf-default-name
2025-10-27 02:00:46,494:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-27 02:00:46,495:INFO:version 3.3.2
2025-10-27 02:00:46,495:INFO:Initializing setup()
2025-10-27 02:00:46,495:INFO:self.USI: 8b16
2025-10-27 02:00:46,495:INFO:self._variable_keys: {'is_multiclass', 'memory', 'seed', 'pipeline', 'exp_id', 'fold_generator', 'fold_shuffle_param', 'y', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'gpu_param', '_available_plots', 'exp_name_log', 'n_jobs_param', 'USI', 'html_param', 'log_plots_param', 'logging_param', 'target_param', 'X_test', 'data', 'y_test', 'fold_groups_param', 'idx', 'y_train', 'X', 'X_train'}
2025-10-27 02:00:46,495:INFO:Checking environment
2025-10-27 02:00:46,495:INFO:python_version: 3.11.14
2025-10-27 02:00:46,495:INFO:python_build: ('main', 'Oct 24 2025 09:18:30')
2025-10-27 02:00:46,495:INFO:machine: x86_64
2025-10-27 02:00:46,495:INFO:platform: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:00:46,495:INFO:Memory: svmem(total=16365076480, available=6004207616, percent=63.3, used=10360868864, free=497229824, active=2374963200, inactive=10751139840, buffers=2609152, cached=7549595648, shared=1381335040, slab=1284263936)
2025-10-27 02:00:46,495:INFO:Physical Core: 12
2025-10-27 02:00:46,495:INFO:Logical Core: 16
2025-10-27 02:00:46,495:INFO:Checking libraries
2025-10-27 02:00:46,495:INFO:System:
2025-10-27 02:00:46,495:INFO:    python: 3.11.14 (main, Oct 24 2025, 09:18:30) [GCC 15.2.1 20250813]
2025-10-27 02:00:46,495:INFO:executable: /home/ykalathiya/pycaret_assignment/.venv/bin/python
2025-10-27 02:00:46,495:INFO:   machine: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:00:46,495:INFO:PyCaret required dependencies:
2025-10-27 02:00:46,495:INFO:                 pip: 25.2
2025-10-27 02:00:46,495:INFO:          setuptools: 80.9.0
2025-10-27 02:00:46,495:INFO:             pycaret: 3.3.2
2025-10-27 02:00:46,495:INFO:             IPython: 9.6.0
2025-10-27 02:00:46,495:INFO:          ipywidgets: 8.1.7
2025-10-27 02:00:46,495:INFO:                tqdm: 4.67.1
2025-10-27 02:00:46,495:INFO:               numpy: 1.26.4
2025-10-27 02:00:46,495:INFO:              pandas: 2.1.4
2025-10-27 02:00:46,495:INFO:              jinja2: 3.1.6
2025-10-27 02:00:46,495:INFO:               scipy: 1.11.4
2025-10-27 02:00:46,495:INFO:              joblib: 1.3.2
2025-10-27 02:00:46,495:INFO:             sklearn: 1.4.2
2025-10-27 02:00:46,495:INFO:                pyod: 2.0.5
2025-10-27 02:00:46,495:INFO:            imblearn: 0.14.0
2025-10-27 02:00:46,495:INFO:   category_encoders: 2.7.0
2025-10-27 02:00:46,495:INFO:            lightgbm: 4.6.0
2025-10-27 02:00:46,495:INFO:               numba: 0.61.0
2025-10-27 02:00:46,495:INFO:            requests: 2.32.5
2025-10-27 02:00:46,495:INFO:          matplotlib: 3.7.5
2025-10-27 02:00:46,495:INFO:          scikitplot: 0.3.7
2025-10-27 02:00:46,495:INFO:         yellowbrick: 1.5
2025-10-27 02:00:46,495:INFO:              plotly: 5.24.1
2025-10-27 02:00:46,495:INFO:    plotly-resampler: Not installed
2025-10-27 02:00:46,495:INFO:             kaleido: 1.1.0
2025-10-27 02:00:46,495:INFO:           schemdraw: 0.15
2025-10-27 02:00:46,495:INFO:         statsmodels: 0.14.5
2025-10-27 02:00:46,495:INFO:              sktime: 0.26.0
2025-10-27 02:00:46,495:INFO:               tbats: 1.1.3
2025-10-27 02:00:46,495:INFO:            pmdarima: 2.0.4
2025-10-27 02:00:46,495:INFO:              psutil: 7.1.1
2025-10-27 02:00:46,496:INFO:          markupsafe: 3.0.3
2025-10-27 02:00:46,496:INFO:             pickle5: Not installed
2025-10-27 02:00:46,496:INFO:         cloudpickle: 3.1.1
2025-10-27 02:00:46,496:INFO:         deprecation: 2.1.0
2025-10-27 02:00:46,496:INFO:              xxhash: 3.6.0
2025-10-27 02:00:46,496:INFO:           wurlitzer: 3.1.1
2025-10-27 02:00:46,496:INFO:PyCaret optional dependencies:
2025-10-27 02:00:46,496:INFO:                shap: 0.44.1
2025-10-27 02:00:46,496:INFO:           interpret: 0.7.3
2025-10-27 02:00:46,496:INFO:                umap: 0.5.7
2025-10-27 02:00:46,496:INFO:     ydata_profiling: 4.17.0
2025-10-27 02:00:46,496:INFO:  explainerdashboard: 0.5.1
2025-10-27 02:00:46,496:INFO:             autoviz: Not installed
2025-10-27 02:00:46,496:INFO:           fairlearn: 0.7.0
2025-10-27 02:00:46,496:INFO:          deepchecks: Not installed
2025-10-27 02:00:46,496:INFO:             xgboost: Not installed
2025-10-27 02:00:46,496:INFO:            catboost: Not installed
2025-10-27 02:00:46,496:INFO:              kmodes: Not installed
2025-10-27 02:00:46,496:INFO:             mlxtend: Not installed
2025-10-27 02:00:46,496:INFO:       statsforecast: Not installed
2025-10-27 02:00:46,496:INFO:        tune_sklearn: Not installed
2025-10-27 02:00:46,496:INFO:                 ray: Not installed
2025-10-27 02:00:46,496:INFO:            hyperopt: Not installed
2025-10-27 02:00:46,496:INFO:              optuna: Not installed
2025-10-27 02:00:46,496:INFO:               skopt: Not installed
2025-10-27 02:00:46,496:INFO:              mlflow: Not installed
2025-10-27 02:00:46,496:INFO:              gradio: Not installed
2025-10-27 02:00:46,496:INFO:             fastapi: Not installed
2025-10-27 02:00:46,496:INFO:             uvicorn: Not installed
2025-10-27 02:00:46,496:INFO:              m2cgen: Not installed
2025-10-27 02:00:46,496:INFO:           evidently: Not installed
2025-10-27 02:00:46,496:INFO:               fugue: Not installed
2025-10-27 02:00:46,496:INFO:           streamlit: Not installed
2025-10-27 02:00:46,496:INFO:             prophet: Not installed
2025-10-27 02:00:46,496:INFO:None
2025-10-27 02:00:46,496:INFO:Set up data.
2025-10-27 02:00:46,499:INFO:Set up folding strategy.
2025-10-27 02:00:46,499:INFO:Set up train/test split.
2025-10-27 02:00:46,502:INFO:Set up index.
2025-10-27 02:00:46,502:INFO:Assigning column types.
2025-10-27 02:00:46,504:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-27 02:00:46,525:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,525:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,538:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,538:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,558:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,558:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,574:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-27 02:00:46,594:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,607:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,607:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,628:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:00:46,641:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,642:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-27 02:00:46,675:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,675:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,709:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,710:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,710:INFO:Preparing preprocessing pipeline...
2025-10-27 02:00:46,711:INFO:Set up simple imputation.
2025-10-27 02:00:46,711:INFO:Set up column transformation.
2025-10-27 02:00:46,711:INFO:Set up feature normalization.
2025-10-27 02:00:46,744:INFO:Finished creating preprocessing pipeline.
2025-10-27 02:00:46,747:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-27 02:00:46,747:INFO:Creating final display dataframe.
2025-10-27 02:00:46,797:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 15)
5   Transformed train set shape        (1674, 15)
6    Transformed test set shape         (718, 15)
7              Numeric features                14
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              8b16
2025-10-27 02:00:46,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,872:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:00:46,873:INFO:setup() successfully completed in 0.38s...............
2025-10-27 02:06:17,165:INFO:Initializing create_model()
2025-10-27 02:06:17,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae5ca28950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:06:17,165:INFO:Checking exceptions
2025-10-27 02:06:17,173:INFO:Importing libraries
2025-10-27 02:06:17,173:INFO:Copying training dataset
2025-10-27 02:06:17,176:INFO:Defining folds
2025-10-27 02:06:17,176:INFO:Declaring metric variables
2025-10-27 02:06:17,178:INFO:Importing untrained model
2025-10-27 02:06:17,180:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:06:17,184:INFO:Starting cross validation
2025-10-27 02:06:17,185:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:06:18,611:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.
2025-10-27 02:06:18,611:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,611:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,611:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,611:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:18,612:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:18,612:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:18,612:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:18,612:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:18,612:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:18,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,615:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.
2025-10-27 02:06:18,615:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,615:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,615:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,615:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:18,615:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:18,616:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:18,616:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:18,616:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:18,616:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:18,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,624:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.
2025-10-27 02:06:18,624:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,624:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,624:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,625:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:18,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,626:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:18,626:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:18,626:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:18,626:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:18,627:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:18,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,642:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001805 seconds.
2025-10-27 02:06:18,642:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,642:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,642:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,642:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:18,643:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:18,643:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:18,643:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:18,643:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:18,643:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:18,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:18,995:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000945 seconds.
2025-10-27 02:06:18,995:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,995:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,995:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,995:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:18,996:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:18,996:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:18,996:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:18,996:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:18,996:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000812 seconds.
2025-10-27 02:06:18,997:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:18,997:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:18,997:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:18,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,074:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000975 seconds.
2025-10-27 02:06:19,074:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-10-27 02:06:19,074:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:19,074:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:19,075:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:19,075:INFO:[LightGBM] [Info] Start training from score -2.182078
2025-10-27 02:06:19,075:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:19,075:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:19,075:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:19,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,126:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001685 seconds.
2025-10-27 02:06:19,126:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:19,126:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:19,126:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:19,126:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:19,126:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:19,127:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:19,127:INFO:[LightGBM] [Info] Start training from score -1.808488
2025-10-27 02:06:19,127:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:19,127:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:19,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,149:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001147 seconds.
2025-10-27 02:06:19,149:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-10-27 02:06:19,149:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:19,149:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:19,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,150:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:19,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,150:INFO:[LightGBM] [Info] Start training from score -2.182078
2025-10-27 02:06:19,151:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:19,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,151:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:19,151:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:19,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000649 seconds.
2025-10-27 02:06:19,168:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Start training from score -3.113184
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Start training from score -1.808488
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:19,168:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:19,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:19,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:26,381:INFO:Initializing create_model()
2025-10-27 02:06:26,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae5ca28950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:06:26,381:INFO:Checking exceptions
2025-10-27 02:06:26,390:INFO:Importing libraries
2025-10-27 02:06:26,390:INFO:Copying training dataset
2025-10-27 02:06:26,393:INFO:Defining folds
2025-10-27 02:06:26,393:INFO:Declaring metric variables
2025-10-27 02:06:26,394:INFO:Importing untrained model
2025-10-27 02:06:26,396:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:06:26,399:INFO:Starting cross validation
2025-10-27 02:06:26,400:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000563 seconds.
2025-10-27 02:06:27,855:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:27,855:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:27,855:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:27,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,933:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.
2025-10-27 02:06:27,933:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:27,934:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:27,934:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:27,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,943:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001487 seconds.
2025-10-27 02:06:27,943:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:27,944:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:27,944:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:27,944:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:27,944:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:27,945:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:27,945:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:27,945:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:27,945:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:27,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,987:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.
2025-10-27 02:06:27,987:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:27,987:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:27,987:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:27,987:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:27,987:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:27,987:INFO:[LightGBM] [Info] Start training from score -2.182078
2025-10-27 02:06:27,988:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:27,988:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:27,988:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:27,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:27,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,230:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003054 seconds.
2025-10-27 02:06:28,230:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,230:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,230:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,230:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:28,230:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:28,231:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:28,231:INFO:[LightGBM] [Info] Start training from score -1.808488
2025-10-27 02:06:28,231:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:28,231:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:28,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,244:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.
2025-10-27 02:06:28,244:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,244:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,244:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,244:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:28,245:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:28,245:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:28,246:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:28,246:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:28,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,246:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:28,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,304:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.
2025-10-27 02:06:28,304:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,305:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Start training from score -3.098368
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Start training from score -2.182078
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Start training from score -1.812545
2025-10-27 02:06:28,305:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:28,306:INFO:[LightGBM] [Info] Start training from score -0.681930
2025-10-27 02:06:28,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,341:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001891 seconds.
2025-10-27 02:06:28,341:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,341:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,341:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,342:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:28,342:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:28,342:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:28,342:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:28,342:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:28,343:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:28,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,406:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001471 seconds.
2025-10-27 02:06:28,407:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,407:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,407:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,407:INFO:[LightGBM] [Info] Number of data points in the train set: 1506, number of used features: 14
2025-10-27 02:06:28,407:INFO:[LightGBM] [Info] Start training from score -3.112520
2025-10-27 02:06:28,407:INFO:[LightGBM] [Info] Start training from score -2.187314
2025-10-27 02:06:28,408:INFO:[LightGBM] [Info] Start training from score -1.807824
2025-10-27 02:06:28,408:INFO:[LightGBM] [Info] Start training from score -1.752692
2025-10-27 02:06:28,408:INFO:[LightGBM] [Info] Start training from score -0.681266
2025-10-27 02:06:28,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,475:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001434 seconds.
2025-10-27 02:06:28,475:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:28,475:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:28,475:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:28,475:INFO:[LightGBM] [Info] Number of data points in the train set: 1507, number of used features: 14
2025-10-27 02:06:28,476:INFO:[LightGBM] [Info] Start training from score -3.113184
2025-10-27 02:06:28,476:INFO:[LightGBM] [Info] Start training from score -2.187977
2025-10-27 02:06:28,476:INFO:[LightGBM] [Info] Start training from score -1.808488
2025-10-27 02:06:28,476:INFO:[LightGBM] [Info] Start training from score -1.753356
2025-10-27 02:06:28,476:INFO:[LightGBM] [Info] Start training from score -0.680618
2025-10-27 02:06:28,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:28,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:29,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:35,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:36,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:37,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,021:INFO:
2025-10-27 02:06:38,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:38,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,099:INFO:Calculating mean and std
2025-10-27 02:06:39,101:INFO:Creating metrics dataframe
2025-10-27 02:06:39,108:INFO:Finalizing model
2025-10-27 02:06:39,156:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000344 seconds.
2025-10-27 02:06:39,156:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:06:39,156:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:06:39,156:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:06:39,156:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 02:06:39,156:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:06:39,157:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:06:39,157:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:06:39,157:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:06:39,157:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:06:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:39,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:06:40,788:INFO:Uploading results into container
2025-10-27 02:06:40,789:INFO:Uploading model into container now
2025-10-27 02:06:40,794:INFO:_master_model_container: 1
2025-10-27 02:06:40,795:INFO:_display_container: 2
2025-10-27 02:06:40,795:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:06:40,795:INFO:create_model() successfully completed......................................
2025-10-27 02:07:13,032:INFO:Initializing create_model()
2025-10-27 02:07:13,032:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae5ca28950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:07:13,032:INFO:Checking exceptions
2025-10-27 02:07:13,040:INFO:Importing libraries
2025-10-27 02:07:13,040:INFO:Copying training dataset
2025-10-27 02:07:13,042:INFO:Defining folds
2025-10-27 02:07:13,042:INFO:Declaring metric variables
2025-10-27 02:07:13,044:INFO:Importing untrained model
2025-10-27 02:07:13,048:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:07:13,051:INFO:Starting cross validation
2025-10-27 02:07:13,051:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:07:24,828:INFO:Calculating mean and std
2025-10-27 02:07:24,829:INFO:Creating metrics dataframe
2025-10-27 02:07:24,832:INFO:Finalizing model
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.
2025-10-27 02:07:24,862:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:07:24,862:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:07:24,862:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:07:24,863:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:07:24,863:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:07:24,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:24,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:25,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:07:26,055:INFO:Uploading results into container
2025-10-27 02:07:26,056:INFO:Uploading model into container now
2025-10-27 02:07:26,061:INFO:_master_model_container: 2
2025-10-27 02:07:26,061:INFO:_display_container: 3
2025-10-27 02:07:26,062:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:07:26,062:INFO:create_model() successfully completed......................................
2025-10-27 02:08:43,922:INFO:Initializing create_model()
2025-10-27 02:08:43,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fae5ca28950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:08:43,922:INFO:Checking exceptions
2025-10-27 02:08:43,928:INFO:Importing libraries
2025-10-27 02:08:43,928:INFO:Copying training dataset
2025-10-27 02:08:43,931:INFO:Defining folds
2025-10-27 02:08:43,931:INFO:Declaring metric variables
2025-10-27 02:08:43,933:INFO:Importing untrained model
2025-10-27 02:08:43,935:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:08:43,939:INFO:Starting cross validation
2025-10-27 02:08:43,940:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:08:54,640:INFO:Calculating mean and std
2025-10-27 02:08:54,641:INFO:Creating metrics dataframe
2025-10-27 02:08:54,645:INFO:Finalizing model
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000302 seconds.
2025-10-27 02:08:54,681:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:08:54,681:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:08:54,681:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:08:54,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:54,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:08:55,878:INFO:Uploading results into container
2025-10-27 02:08:55,878:INFO:Uploading model into container now
2025-10-27 02:08:55,885:INFO:_master_model_container: 3
2025-10-27 02:08:55,885:INFO:_display_container: 4
2025-10-27 02:08:55,885:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:08:55,885:INFO:create_model() successfully completed......................................
2025-10-27 02:11:21,581:ERROR:
'gradio' is a soft dependency and not included in the pycaret installation. Please run: `pip install gradio` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-10-27 02:14:16,007:ERROR:
'gradio' is a soft dependency and not included in the pycaret installation. Please run: `pip install gradio` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-10-27 02:15:10,533:ERROR:
'gradio' is a soft dependency and not included in the pycaret installation. Please run: `pip install gradio` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-10-27 02:15:54,036:ERROR:
'gradio' is a soft dependency and not included in the pycaret installation. Please run: `pip install gradio` to install.
Alternately, you can install this by running `pip install pycaret[mlops]`
NoneType: None
2025-10-27 02:16:15,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 02:16:15,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 02:16:15,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 02:16:15,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-27 02:16:16,127:INFO:PyCaret ClassificationExperiment
2025-10-27 02:16:16,127:INFO:Logging name: clf-default-name
2025-10-27 02:16:16,127:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-27 02:16:16,127:INFO:version 3.3.2
2025-10-27 02:16:16,127:INFO:Initializing setup()
2025-10-27 02:16:16,127:INFO:self.USI: 2e9c
2025-10-27 02:16:16,127:INFO:self._variable_keys: {'fix_imbalance', 'fold_generator', 'X', 'idx', 'target_param', '_ml_usecase', 'exp_name_log', 'pipeline', 'memory', 'html_param', 'X_train', 'fold_groups_param', 'y_train', 'gpu_n_jobs_param', 'gpu_param', 'fold_shuffle_param', 'y', 'data', '_available_plots', 'log_plots_param', 'X_test', 'seed', 'USI', 'exp_id', 'logging_param', 'y_test', 'is_multiclass', 'n_jobs_param'}
2025-10-27 02:16:16,127:INFO:Checking environment
2025-10-27 02:16:16,127:INFO:python_version: 3.11.14
2025-10-27 02:16:16,127:INFO:python_build: ('main', 'Oct 24 2025 09:18:30')
2025-10-27 02:16:16,128:INFO:machine: x86_64
2025-10-27 02:16:16,128:INFO:platform: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:16:16,128:INFO:Memory: svmem(total=16365076480, available=7190376448, percent=56.1, used=9174700032, free=965111808, active=6118219776, inactive=6572257280, buffers=24576, cached=8408121344, shared=1518538752, slab=1309671424)
2025-10-27 02:16:16,128:INFO:Physical Core: 12
2025-10-27 02:16:16,128:INFO:Logical Core: 16
2025-10-27 02:16:16,128:INFO:Checking libraries
2025-10-27 02:16:16,128:INFO:System:
2025-10-27 02:16:16,128:INFO:    python: 3.11.14 (main, Oct 24 2025, 09:18:30) [GCC 15.2.1 20250813]
2025-10-27 02:16:16,128:INFO:executable: /home/ykalathiya/pycaret_assignment/.venv/bin/python
2025-10-27 02:16:16,128:INFO:   machine: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:16:16,128:INFO:PyCaret required dependencies:
2025-10-27 02:16:16,171:INFO:                 pip: 25.2
2025-10-27 02:16:16,171:INFO:          setuptools: 80.9.0
2025-10-27 02:16:16,171:INFO:             pycaret: 3.3.2
2025-10-27 02:16:16,171:INFO:             IPython: 9.6.0
2025-10-27 02:16:16,171:INFO:          ipywidgets: 8.1.7
2025-10-27 02:16:16,171:INFO:                tqdm: 4.67.1
2025-10-27 02:16:16,171:INFO:               numpy: 1.26.4
2025-10-27 02:16:16,171:INFO:              pandas: 2.1.4
2025-10-27 02:16:16,171:INFO:              jinja2: 3.1.6
2025-10-27 02:16:16,171:INFO:               scipy: 1.11.4
2025-10-27 02:16:16,171:INFO:              joblib: 1.3.2
2025-10-27 02:16:16,171:INFO:             sklearn: 1.4.2
2025-10-27 02:16:16,171:INFO:                pyod: 2.0.5
2025-10-27 02:16:16,171:INFO:            imblearn: 0.14.0
2025-10-27 02:16:16,171:INFO:   category_encoders: 2.7.0
2025-10-27 02:16:16,171:INFO:            lightgbm: 4.6.0
2025-10-27 02:16:16,171:INFO:               numba: 0.61.0
2025-10-27 02:16:16,171:INFO:            requests: 2.32.5
2025-10-27 02:16:16,171:INFO:          matplotlib: 3.7.5
2025-10-27 02:16:16,171:INFO:          scikitplot: 0.3.7
2025-10-27 02:16:16,171:INFO:         yellowbrick: 1.5
2025-10-27 02:16:16,171:INFO:              plotly: 5.24.1
2025-10-27 02:16:16,171:INFO:    plotly-resampler: Not installed
2025-10-27 02:16:16,172:INFO:             kaleido: 1.1.0
2025-10-27 02:16:16,172:INFO:           schemdraw: 0.15
2025-10-27 02:16:16,172:INFO:         statsmodels: 0.14.5
2025-10-27 02:16:16,172:INFO:              sktime: 0.26.0
2025-10-27 02:16:16,172:INFO:               tbats: 1.1.3
2025-10-27 02:16:16,172:INFO:            pmdarima: 2.0.4
2025-10-27 02:16:16,172:INFO:              psutil: 7.1.1
2025-10-27 02:16:16,172:INFO:          markupsafe: 3.0.3
2025-10-27 02:16:16,172:INFO:             pickle5: Not installed
2025-10-27 02:16:16,172:INFO:         cloudpickle: 3.1.1
2025-10-27 02:16:16,172:INFO:         deprecation: 2.1.0
2025-10-27 02:16:16,172:INFO:              xxhash: 3.6.0
2025-10-27 02:16:16,172:INFO:           wurlitzer: 3.1.1
2025-10-27 02:16:16,172:INFO:PyCaret optional dependencies:
2025-10-27 02:16:17,924:INFO:                shap: 0.44.1
2025-10-27 02:16:17,924:INFO:           interpret: 0.7.3
2025-10-27 02:16:17,924:INFO:                umap: 0.5.7
2025-10-27 02:16:17,924:INFO:     ydata_profiling: 4.17.0
2025-10-27 02:16:17,924:INFO:  explainerdashboard: 0.5.1
2025-10-27 02:16:17,924:INFO:             autoviz: Not installed
2025-10-27 02:16:17,924:INFO:           fairlearn: 0.7.0
2025-10-27 02:16:17,924:INFO:          deepchecks: Not installed
2025-10-27 02:16:17,924:INFO:             xgboost: Not installed
2025-10-27 02:16:17,924:INFO:            catboost: Not installed
2025-10-27 02:16:17,924:INFO:              kmodes: Not installed
2025-10-27 02:16:17,924:INFO:             mlxtend: Not installed
2025-10-27 02:16:17,924:INFO:       statsforecast: Not installed
2025-10-27 02:16:17,924:INFO:        tune_sklearn: Not installed
2025-10-27 02:16:17,925:INFO:                 ray: Not installed
2025-10-27 02:16:17,925:INFO:            hyperopt: Not installed
2025-10-27 02:16:17,925:INFO:              optuna: Not installed
2025-10-27 02:16:17,925:INFO:               skopt: Not installed
2025-10-27 02:16:17,925:INFO:              mlflow: 3.5.1
2025-10-27 02:16:17,925:INFO:              gradio: 5.49.1
2025-10-27 02:16:17,925:INFO:             fastapi: 0.120.0
2025-10-27 02:16:17,925:INFO:             uvicorn: 0.38.0
2025-10-27 02:16:17,925:INFO:              m2cgen: 0.10.0
2025-10-27 02:16:17,925:INFO:           evidently: 0.4.40
2025-10-27 02:16:17,925:INFO:               fugue: Not installed
2025-10-27 02:16:17,925:INFO:           streamlit: Not installed
2025-10-27 02:16:17,925:INFO:             prophet: Not installed
2025-10-27 02:16:17,925:INFO:None
2025-10-27 02:16:17,925:INFO:Set up data.
2025-10-27 02:16:17,929:INFO:Set up folding strategy.
2025-10-27 02:16:17,929:INFO:Set up train/test split.
2025-10-27 02:16:17,931:INFO:Set up index.
2025-10-27 02:16:17,932:INFO:Assigning column types.
2025-10-27 02:16:17,934:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-27 02:16:17,957:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:16:17,958:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:16:17,976:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:17,977:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:17,999:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:16:18,000:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:16:18,014:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,014:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,016:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-27 02:16:18,039:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:16:18,054:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,054:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,079:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:16:18,094:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,094:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,095:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-27 02:16:18,134:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,134:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,173:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,174:INFO:Preparing preprocessing pipeline...
2025-10-27 02:16:18,174:INFO:Set up simple imputation.
2025-10-27 02:16:18,174:INFO:Set up column transformation.
2025-10-27 02:16:18,175:INFO:Set up feature normalization.
2025-10-27 02:16:18,210:INFO:Finished creating preprocessing pipeline.
2025-10-27 02:16:18,214:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-27 02:16:18,214:INFO:Creating final display dataframe.
2025-10-27 02:16:18,267:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 15)
5   Transformed train set shape        (1674, 15)
6    Transformed test set shape         (718, 15)
7              Numeric features                14
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              2e9c
2025-10-27 02:16:18,308:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,309:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,351:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:16:18,352:INFO:setup() successfully completed in 2.23s...............
2025-10-27 02:16:18,361:INFO:Initializing compare_models()
2025-10-27 02:16:18,362:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-10-27 02:16:18,362:INFO:Checking exceptions
2025-10-27 02:16:18,365:INFO:Preparing display monitor
2025-10-27 02:16:18,382:INFO:Initializing Logistic Regression
2025-10-27 02:16:18,382:INFO:Total runtime is 2.3643175760904947e-06 minutes
2025-10-27 02:16:18,385:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:18,385:INFO:Initializing create_model()
2025-10-27 02:16:18,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:18,385:INFO:Checking exceptions
2025-10-27 02:16:18,385:INFO:Importing libraries
2025-10-27 02:16:18,385:INFO:Copying training dataset
2025-10-27 02:16:18,388:INFO:Defining folds
2025-10-27 02:16:18,388:INFO:Declaring metric variables
2025-10-27 02:16:18,390:INFO:Importing untrained model
2025-10-27 02:16:18,392:INFO:Logistic Regression Imported successfully
2025-10-27 02:16:18,398:INFO:Starting cross validation
2025-10-27 02:16:18,399:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:20,131:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,167:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,199:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,199:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,481:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,546:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,576:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,579:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:20,612:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,661:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,666:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:20,681:INFO:Calculating mean and std
2025-10-27 02:16:20,683:INFO:Creating metrics dataframe
2025-10-27 02:16:20,687:INFO:Uploading results into container
2025-10-27 02:16:20,687:INFO:Uploading model into container now
2025-10-27 02:16:20,688:INFO:_master_model_container: 1
2025-10-27 02:16:20,688:INFO:_display_container: 2
2025-10-27 02:16:20,689:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-27 02:16:20,689:INFO:create_model() successfully completed......................................
2025-10-27 02:16:20,829:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:20,830:INFO:Creating metrics dataframe
2025-10-27 02:16:20,834:INFO:Initializing K Neighbors Classifier
2025-10-27 02:16:20,834:INFO:Total runtime is 0.04086565574010213 minutes
2025-10-27 02:16:20,836:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:20,837:INFO:Initializing create_model()
2025-10-27 02:16:20,837:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:20,837:INFO:Checking exceptions
2025-10-27 02:16:20,837:INFO:Importing libraries
2025-10-27 02:16:20,837:INFO:Copying training dataset
2025-10-27 02:16:20,840:INFO:Defining folds
2025-10-27 02:16:20,840:INFO:Declaring metric variables
2025-10-27 02:16:20,842:INFO:Importing untrained model
2025-10-27 02:16:20,845:INFO:K Neighbors Classifier Imported successfully
2025-10-27 02:16:20,848:INFO:Starting cross validation
2025-10-27 02:16:20,849:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:20,993:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,452:INFO:Calculating mean and std
2025-10-27 02:16:22,453:INFO:Creating metrics dataframe
2025-10-27 02:16:22,456:INFO:Uploading results into container
2025-10-27 02:16:22,457:INFO:Uploading model into container now
2025-10-27 02:16:22,458:INFO:_master_model_container: 2
2025-10-27 02:16:22,458:INFO:_display_container: 2
2025-10-27 02:16:22,458:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-27 02:16:22,458:INFO:create_model() successfully completed......................................
2025-10-27 02:16:22,598:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:22,598:INFO:Creating metrics dataframe
2025-10-27 02:16:22,603:INFO:Initializing Naive Bayes
2025-10-27 02:16:22,603:INFO:Total runtime is 0.07034782965977987 minutes
2025-10-27 02:16:22,605:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:22,605:INFO:Initializing create_model()
2025-10-27 02:16:22,605:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:22,605:INFO:Checking exceptions
2025-10-27 02:16:22,605:INFO:Importing libraries
2025-10-27 02:16:22,606:INFO:Copying training dataset
2025-10-27 02:16:22,609:INFO:Defining folds
2025-10-27 02:16:22,609:INFO:Declaring metric variables
2025-10-27 02:16:22,611:INFO:Importing untrained model
2025-10-27 02:16:22,613:INFO:Naive Bayes Imported successfully
2025-10-27 02:16:22,617:INFO:Starting cross validation
2025-10-27 02:16:22,617:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:22,680:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,691:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,696:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,716:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,721:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,723:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:22,736:INFO:Calculating mean and std
2025-10-27 02:16:22,737:INFO:Creating metrics dataframe
2025-10-27 02:16:22,738:INFO:Uploading results into container
2025-10-27 02:16:22,739:INFO:Uploading model into container now
2025-10-27 02:16:22,739:INFO:_master_model_container: 3
2025-10-27 02:16:22,739:INFO:_display_container: 2
2025-10-27 02:16:22,739:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-27 02:16:22,739:INFO:create_model() successfully completed......................................
2025-10-27 02:16:22,852:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:22,852:INFO:Creating metrics dataframe
2025-10-27 02:16:22,856:INFO:Initializing Decision Tree Classifier
2025-10-27 02:16:22,857:INFO:Total runtime is 0.07457103331883749 minutes
2025-10-27 02:16:22,859:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:22,859:INFO:Initializing create_model()
2025-10-27 02:16:22,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:22,859:INFO:Checking exceptions
2025-10-27 02:16:22,859:INFO:Importing libraries
2025-10-27 02:16:22,859:INFO:Copying training dataset
2025-10-27 02:16:22,862:INFO:Defining folds
2025-10-27 02:16:22,863:INFO:Declaring metric variables
2025-10-27 02:16:22,864:INFO:Importing untrained model
2025-10-27 02:16:22,866:INFO:Decision Tree Classifier Imported successfully
2025-10-27 02:16:22,870:INFO:Starting cross validation
2025-10-27 02:16:22,870:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:22,991:INFO:Calculating mean and std
2025-10-27 02:16:22,992:INFO:Creating metrics dataframe
2025-10-27 02:16:22,995:INFO:Uploading results into container
2025-10-27 02:16:22,995:INFO:Uploading model into container now
2025-10-27 02:16:22,996:INFO:_master_model_container: 4
2025-10-27 02:16:22,996:INFO:_display_container: 2
2025-10-27 02:16:22,996:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-10-27 02:16:22,996:INFO:create_model() successfully completed......................................
2025-10-27 02:16:23,106:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:23,107:INFO:Creating metrics dataframe
2025-10-27 02:16:23,111:INFO:Initializing SVM - Linear Kernel
2025-10-27 02:16:23,112:INFO:Total runtime is 0.0788221557935079 minutes
2025-10-27 02:16:23,113:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:23,114:INFO:Initializing create_model()
2025-10-27 02:16:23,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:23,114:INFO:Checking exceptions
2025-10-27 02:16:23,114:INFO:Importing libraries
2025-10-27 02:16:23,114:INFO:Copying training dataset
2025-10-27 02:16:23,117:INFO:Defining folds
2025-10-27 02:16:23,117:INFO:Declaring metric variables
2025-10-27 02:16:23,119:INFO:Importing untrained model
2025-10-27 02:16:23,121:INFO:SVM - Linear Kernel Imported successfully
2025-10-27 02:16:23,125:INFO:Starting cross validation
2025-10-27 02:16:23,126:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:23,202:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,205:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,215:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,232:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,243:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,248:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,250:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,253:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,259:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,262:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,268:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,271:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,289:INFO:Calculating mean and std
2025-10-27 02:16:23,290:INFO:Creating metrics dataframe
2025-10-27 02:16:23,291:INFO:Uploading results into container
2025-10-27 02:16:23,291:INFO:Uploading model into container now
2025-10-27 02:16:23,292:INFO:_master_model_container: 5
2025-10-27 02:16:23,292:INFO:_display_container: 2
2025-10-27 02:16:23,292:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-27 02:16:23,292:INFO:create_model() successfully completed......................................
2025-10-27 02:16:23,402:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:23,402:INFO:Creating metrics dataframe
2025-10-27 02:16:23,407:INFO:Initializing Ridge Classifier
2025-10-27 02:16:23,407:INFO:Total runtime is 0.08375128110249838 minutes
2025-10-27 02:16:23,410:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:23,411:INFO:Initializing create_model()
2025-10-27 02:16:23,411:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:23,411:INFO:Checking exceptions
2025-10-27 02:16:23,411:INFO:Importing libraries
2025-10-27 02:16:23,411:INFO:Copying training dataset
2025-10-27 02:16:23,414:INFO:Defining folds
2025-10-27 02:16:23,414:INFO:Declaring metric variables
2025-10-27 02:16:23,416:INFO:Importing untrained model
2025-10-27 02:16:23,419:INFO:Ridge Classifier Imported successfully
2025-10-27 02:16:23,423:INFO:Starting cross validation
2025-10-27 02:16:23,424:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:23,477:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,477:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,480:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,480:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,499:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,502:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,524:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,525:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,526:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,529:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,530:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,531:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,531:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,532:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,532:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,532:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:23,536:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,537:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,537:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,540:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:23,554:INFO:Calculating mean and std
2025-10-27 02:16:23,555:INFO:Creating metrics dataframe
2025-10-27 02:16:23,557:INFO:Uploading results into container
2025-10-27 02:16:23,557:INFO:Uploading model into container now
2025-10-27 02:16:23,558:INFO:_master_model_container: 6
2025-10-27 02:16:23,558:INFO:_display_container: 2
2025-10-27 02:16:23,558:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-10-27 02:16:23,558:INFO:create_model() successfully completed......................................
2025-10-27 02:16:23,687:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:23,688:INFO:Creating metrics dataframe
2025-10-27 02:16:23,693:INFO:Initializing Random Forest Classifier
2025-10-27 02:16:23,693:INFO:Total runtime is 0.08851234118143718 minutes
2025-10-27 02:16:23,695:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:23,695:INFO:Initializing create_model()
2025-10-27 02:16:23,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:23,695:INFO:Checking exceptions
2025-10-27 02:16:23,695:INFO:Importing libraries
2025-10-27 02:16:23,695:INFO:Copying training dataset
2025-10-27 02:16:23,699:INFO:Defining folds
2025-10-27 02:16:23,699:INFO:Declaring metric variables
2025-10-27 02:16:23,701:INFO:Importing untrained model
2025-10-27 02:16:23,705:INFO:Random Forest Classifier Imported successfully
2025-10-27 02:16:23,712:INFO:Starting cross validation
2025-10-27 02:16:23,713:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:24,141:INFO:Calculating mean and std
2025-10-27 02:16:24,143:INFO:Creating metrics dataframe
2025-10-27 02:16:24,145:INFO:Uploading results into container
2025-10-27 02:16:24,145:INFO:Uploading model into container now
2025-10-27 02:16:24,146:INFO:_master_model_container: 7
2025-10-27 02:16:24,146:INFO:_display_container: 2
2025-10-27 02:16:24,146:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-10-27 02:16:24,147:INFO:create_model() successfully completed......................................
2025-10-27 02:16:24,251:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:24,251:INFO:Creating metrics dataframe
2025-10-27 02:16:24,255:INFO:Initializing Quadratic Discriminant Analysis
2025-10-27 02:16:24,255:INFO:Total runtime is 0.09788667758305868 minutes
2025-10-27 02:16:24,257:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:24,258:INFO:Initializing create_model()
2025-10-27 02:16:24,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:24,258:INFO:Checking exceptions
2025-10-27 02:16:24,258:INFO:Importing libraries
2025-10-27 02:16:24,258:INFO:Copying training dataset
2025-10-27 02:16:24,262:INFO:Defining folds
2025-10-27 02:16:24,262:INFO:Declaring metric variables
2025-10-27 02:16:24,264:INFO:Importing untrained model
2025-10-27 02:16:24,265:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-27 02:16:24,268:INFO:Starting cross validation
2025-10-27 02:16:24,269:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:24,312:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,322:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,326:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,327:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,341:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,347:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,348:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,355:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,359:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,360:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,367:INFO:Calculating mean and std
2025-10-27 02:16:24,367:INFO:Creating metrics dataframe
2025-10-27 02:16:24,368:INFO:Uploading results into container
2025-10-27 02:16:24,369:INFO:Uploading model into container now
2025-10-27 02:16:24,369:INFO:_master_model_container: 8
2025-10-27 02:16:24,369:INFO:_display_container: 2
2025-10-27 02:16:24,369:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-27 02:16:24,369:INFO:create_model() successfully completed......................................
2025-10-27 02:16:24,476:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:24,476:INFO:Creating metrics dataframe
2025-10-27 02:16:24,481:INFO:Initializing Ada Boost Classifier
2025-10-27 02:16:24,481:INFO:Total runtime is 0.1016432523727417 minutes
2025-10-27 02:16:24,483:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:24,483:INFO:Initializing create_model()
2025-10-27 02:16:24,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:24,483:INFO:Checking exceptions
2025-10-27 02:16:24,483:INFO:Importing libraries
2025-10-27 02:16:24,483:INFO:Copying training dataset
2025-10-27 02:16:24,486:INFO:Defining folds
2025-10-27 02:16:24,486:INFO:Declaring metric variables
2025-10-27 02:16:24,488:INFO:Importing untrained model
2025-10-27 02:16:24,490:INFO:Ada Boost Classifier Imported successfully
2025-10-27 02:16:24,493:INFO:Starting cross validation
2025-10-27 02:16:24,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:24,531:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,533:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,534:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,546:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,559:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,565:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,570:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,570:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,571:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,583:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:16:24,646:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,646:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,663:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,673:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,715:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,723:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,723:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,741:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,743:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,753:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:24,763:INFO:Calculating mean and std
2025-10-27 02:16:24,764:INFO:Creating metrics dataframe
2025-10-27 02:16:24,766:INFO:Uploading results into container
2025-10-27 02:16:24,767:INFO:Uploading model into container now
2025-10-27 02:16:24,767:INFO:_master_model_container: 9
2025-10-27 02:16:24,767:INFO:_display_container: 2
2025-10-27 02:16:24,768:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-10-27 02:16:24,768:INFO:create_model() successfully completed......................................
2025-10-27 02:16:24,880:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:24,880:INFO:Creating metrics dataframe
2025-10-27 02:16:24,885:INFO:Initializing Gradient Boosting Classifier
2025-10-27 02:16:24,886:INFO:Total runtime is 0.10838824510574341 minutes
2025-10-27 02:16:24,888:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:24,888:INFO:Initializing create_model()
2025-10-27 02:16:24,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:24,888:INFO:Checking exceptions
2025-10-27 02:16:24,888:INFO:Importing libraries
2025-10-27 02:16:24,888:INFO:Copying training dataset
2025-10-27 02:16:24,891:INFO:Defining folds
2025-10-27 02:16:24,892:INFO:Declaring metric variables
2025-10-27 02:16:24,894:INFO:Importing untrained model
2025-10-27 02:16:24,896:INFO:Gradient Boosting Classifier Imported successfully
2025-10-27 02:16:24,900:INFO:Starting cross validation
2025-10-27 02:16:24,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:26,321:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,323:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,367:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,424:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,770:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,777:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,785:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,815:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,925:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,950:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:26,966:INFO:Calculating mean and std
2025-10-27 02:16:26,967:INFO:Creating metrics dataframe
2025-10-27 02:16:26,968:INFO:Uploading results into container
2025-10-27 02:16:26,969:INFO:Uploading model into container now
2025-10-27 02:16:26,969:INFO:_master_model_container: 10
2025-10-27 02:16:26,969:INFO:_display_container: 2
2025-10-27 02:16:26,969:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-27 02:16:26,969:INFO:create_model() successfully completed......................................
2025-10-27 02:16:27,071:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:27,071:INFO:Creating metrics dataframe
2025-10-27 02:16:27,076:INFO:Initializing Linear Discriminant Analysis
2025-10-27 02:16:27,076:INFO:Total runtime is 0.1448997894922892 minutes
2025-10-27 02:16:27,078:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:27,078:INFO:Initializing create_model()
2025-10-27 02:16:27,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:27,078:INFO:Checking exceptions
2025-10-27 02:16:27,078:INFO:Importing libraries
2025-10-27 02:16:27,078:INFO:Copying training dataset
2025-10-27 02:16:27,080:INFO:Defining folds
2025-10-27 02:16:27,081:INFO:Declaring metric variables
2025-10-27 02:16:27,082:INFO:Importing untrained model
2025-10-27 02:16:27,084:INFO:Linear Discriminant Analysis Imported successfully
2025-10-27 02:16:27,086:INFO:Starting cross validation
2025-10-27 02:16:27,087:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:27,130:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,133:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:27,137:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,139:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,139:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,143:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:27,156:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,160:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,165:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,165:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,172:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,173:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:16:27,184:INFO:Calculating mean and std
2025-10-27 02:16:27,185:INFO:Creating metrics dataframe
2025-10-27 02:16:27,186:INFO:Uploading results into container
2025-10-27 02:16:27,186:INFO:Uploading model into container now
2025-10-27 02:16:27,186:INFO:_master_model_container: 11
2025-10-27 02:16:27,187:INFO:_display_container: 2
2025-10-27 02:16:27,187:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-27 02:16:27,187:INFO:create_model() successfully completed......................................
2025-10-27 02:16:27,291:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:27,291:INFO:Creating metrics dataframe
2025-10-27 02:16:27,296:INFO:Initializing Extra Trees Classifier
2025-10-27 02:16:27,296:INFO:Total runtime is 0.14855510393778482 minutes
2025-10-27 02:16:27,297:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:27,297:INFO:Initializing create_model()
2025-10-27 02:16:27,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:27,297:INFO:Checking exceptions
2025-10-27 02:16:27,297:INFO:Importing libraries
2025-10-27 02:16:27,297:INFO:Copying training dataset
2025-10-27 02:16:27,300:INFO:Defining folds
2025-10-27 02:16:27,300:INFO:Declaring metric variables
2025-10-27 02:16:27,301:INFO:Importing untrained model
2025-10-27 02:16:27,303:INFO:Extra Trees Classifier Imported successfully
2025-10-27 02:16:27,308:INFO:Starting cross validation
2025-10-27 02:16:27,309:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:27,666:INFO:Calculating mean and std
2025-10-27 02:16:27,667:INFO:Creating metrics dataframe
2025-10-27 02:16:27,669:INFO:Uploading results into container
2025-10-27 02:16:27,669:INFO:Uploading model into container now
2025-10-27 02:16:27,670:INFO:_master_model_container: 12
2025-10-27 02:16:27,670:INFO:_display_container: 2
2025-10-27 02:16:27,670:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-10-27 02:16:27,670:INFO:create_model() successfully completed......................................
2025-10-27 02:16:27,773:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:27,773:INFO:Creating metrics dataframe
2025-10-27 02:16:27,778:INFO:Initializing Light Gradient Boosting Machine
2025-10-27 02:16:27,778:INFO:Total runtime is 0.15659926732381182 minutes
2025-10-27 02:16:27,780:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:27,780:INFO:Initializing create_model()
2025-10-27 02:16:27,780:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:27,780:INFO:Checking exceptions
2025-10-27 02:16:27,780:INFO:Importing libraries
2025-10-27 02:16:27,780:INFO:Copying training dataset
2025-10-27 02:16:27,783:INFO:Defining folds
2025-10-27 02:16:27,783:INFO:Declaring metric variables
2025-10-27 02:16:27,785:INFO:Importing untrained model
2025-10-27 02:16:27,787:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:16:27,790:INFO:Starting cross validation
2025-10-27 02:16:27,790:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:38,372:WARNING:Exception in thread Thread-48:
2025-10-27 02:16:38,372:WARNING:Traceback (most recent call last):
2025-10-27 02:16:38,372:WARNING:  File "/usr/lib/python3.11/threading.py", line 1045, in _bootstrap_inner
2025-10-27 02:16:38,372:WARNING:    self.run()
2025-10-27 02:16:38,372:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 84, in run
2025-10-27 02:16:38,373:WARNING:    instance.refresh(nolock=True)
2025-10-27 02:16:38,373:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/std.py", line 1347, in refresh
2025-10-27 02:16:38,375:WARNING:    self.display()
2025-10-27 02:16:38,375:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/display/progress_bar.py", line 71, in display
2025-10-27 02:16:38,376:WARNING:    super().display(msg, pos, close, bar_style, check_delay)
2025-10-27 02:16:38,376:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/notebook.py", line 157, in display
2025-10-27 02:16:38,376:WARNING:    pbar.value = self.n
2025-10-27 02:16:38,376:WARNING:    ^^^^^^^^^^
2025-10-27 02:16:38,376:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 716, in __set__
2025-10-27 02:16:38,376:WARNING:    self.set(obj, value)
2025-10-27 02:16:38,376:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 706, in set
2025-10-27 02:16:38,376:WARNING:    obj._notify_trait(self.name, old_value, new_value)
2025-10-27 02:16:38,376:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 1513, in _notify_trait
2025-10-27 02:16:38,377:WARNING:    self.notify_change(
2025-10-27 02:16:38,377:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 700, in notify_change
2025-10-27 02:16:38,377:WARNING:    self.send_state(key=name)
2025-10-27 02:16:38,377:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 586, in send_state
2025-10-27 02:16:38,377:WARNING:    self._send(msg, buffers=buffers)
2025-10-27 02:16:38,377:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 825, in _send
2025-10-27 02:16:38,377:WARNING:    self.comm.send(data=msg, buffers=buffers)
2025-10-27 02:16:38,377:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/comm/base_comm.py", line 144, in send
2025-10-27 02:16:38,378:WARNING:    self.publish_msg(
2025-10-27 02:16:38,378:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipykernel/comm/comm.py", line 42, in publish_msg
2025-10-27 02:16:38,378:WARNING:    parent=self.kernel.get_parent(),
2025-10-27 02:16:38,378:WARNING:           ^^^^^^^^^^^^^^^^^^^^^^^^
2025-10-27 02:16:38,378:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py", line 797, in get_parent
2025-10-27 02:16:38,380:WARNING:    return self._shell_parent.get()
2025-10-27 02:16:38,380:WARNING:           ^^^^^^^^^^^^^^^^^^^^^^^^
2025-10-27 02:16:38,380:WARNING:LookupError: <ContextVar name='shell_parent' at 0x7f1bcec2db20>
2025-10-27 02:16:40,354:INFO:Calculating mean and std
2025-10-27 02:16:40,355:INFO:Creating metrics dataframe
2025-10-27 02:16:40,358:INFO:Uploading results into container
2025-10-27 02:16:40,359:INFO:Uploading model into container now
2025-10-27 02:16:40,359:INFO:_master_model_container: 13
2025-10-27 02:16:40,359:INFO:_display_container: 2
2025-10-27 02:16:40,360:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:16:40,360:INFO:create_model() successfully completed......................................
2025-10-27 02:16:40,489:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:40,489:INFO:Creating metrics dataframe
2025-10-27 02:16:40,495:INFO:Initializing Dummy Classifier
2025-10-27 02:16:40,495:INFO:Total runtime is 0.3685414393742879 minutes
2025-10-27 02:16:40,496:INFO:SubProcess create_model() called ==================================
2025-10-27 02:16:40,496:INFO:Initializing create_model()
2025-10-27 02:16:40,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1b41d76910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:40,496:INFO:Checking exceptions
2025-10-27 02:16:40,496:INFO:Importing libraries
2025-10-27 02:16:40,496:INFO:Copying training dataset
2025-10-27 02:16:40,499:INFO:Defining folds
2025-10-27 02:16:40,500:INFO:Declaring metric variables
2025-10-27 02:16:40,501:INFO:Importing untrained model
2025-10-27 02:16:40,502:INFO:Dummy Classifier Imported successfully
2025-10-27 02:16:40,505:INFO:Starting cross validation
2025-10-27 02:16:40,506:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:40,583:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,585:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,585:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,585:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,593:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,601:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,607:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,608:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,608:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,608:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:16:40,614:INFO:Calculating mean and std
2025-10-27 02:16:40,615:INFO:Creating metrics dataframe
2025-10-27 02:16:40,617:INFO:Uploading results into container
2025-10-27 02:16:40,618:INFO:Uploading model into container now
2025-10-27 02:16:40,618:INFO:_master_model_container: 14
2025-10-27 02:16:40,618:INFO:_display_container: 2
2025-10-27 02:16:40,618:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-10-27 02:16:40,618:INFO:create_model() successfully completed......................................
2025-10-27 02:16:40,720:INFO:SubProcess create_model() end ==================================
2025-10-27 02:16:40,720:INFO:Creating metrics dataframe
2025-10-27 02:16:40,726:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2025-10-27 02:16:40,730:INFO:Initializing create_model()
2025-10-27 02:16:40,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:40,731:INFO:Checking exceptions
2025-10-27 02:16:40,732:INFO:Importing libraries
2025-10-27 02:16:40,732:INFO:Copying training dataset
2025-10-27 02:16:40,734:INFO:Defining folds
2025-10-27 02:16:40,734:INFO:Declaring metric variables
2025-10-27 02:16:40,734:INFO:Importing untrained model
2025-10-27 02:16:40,734:INFO:Declaring custom model
2025-10-27 02:16:40,735:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:16:40,735:INFO:Cross validation set to False
2025-10-27 02:16:40,735:INFO:Fitting Model
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000257 seconds.
2025-10-27 02:16:40,766:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:16:40,766:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:16:40,766:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:16:40,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:40,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:41,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:42,111:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:16:42,112:INFO:create_model() successfully completed......................................
2025-10-27 02:16:42,235:INFO:_master_model_container: 14
2025-10-27 02:16:42,235:INFO:_display_container: 2
2025-10-27 02:16:42,236:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:16:42,236:INFO:compare_models() successfully completed......................................
2025-10-27 02:16:42,253:INFO:Initializing evaluate_model()
2025-10-27 02:16:42,253:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-10-27 02:16:42,266:INFO:Initializing plot_model()
2025-10-27 02:16:42,266:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-10-27 02:16:42,266:INFO:Checking exceptions
2025-10-27 02:16:42,268:INFO:Preloading libraries
2025-10-27 02:16:42,288:INFO:Copying training dataset
2025-10-27 02:16:42,288:INFO:Plot type: pipeline
2025-10-27 02:16:42,417:INFO:Visual Rendered Successfully
2025-10-27 02:16:42,560:INFO:plot_model() successfully completed......................................
2025-10-27 02:16:42,586:INFO:Initializing save_model()
2025-10-27 02:16:42,586:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=best_student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fi...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-10-27 02:16:42,586:INFO:Adding model into prep_pipe
2025-10-27 02:16:42,606:INFO:best_student_performance_model.pkl saved in current working directory
2025-10-27 02:16:42,613:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['StudentID', 'Age', 'Gender',
                                             'Ethnicity', 'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=No...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=123,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-10-27 02:16:42,613:INFO:save_model() successfully completed......................................
2025-10-27 02:16:42,733:INFO:Initializing predict_model()
2025-10-27 02:16:42,733:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1b41afb380>)
2025-10-27 02:16:42,733:INFO:Checking exceptions
2025-10-27 02:16:42,734:INFO:Preloading libraries
2025-10-27 02:16:42,924:INFO:Initializing create_model()
2025-10-27 02:16:42,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:16:42,925:INFO:Checking exceptions
2025-10-27 02:16:42,940:INFO:Importing libraries
2025-10-27 02:16:42,940:INFO:Copying training dataset
2025-10-27 02:16:42,947:INFO:Defining folds
2025-10-27 02:16:42,947:INFO:Declaring metric variables
2025-10-27 02:16:42,949:INFO:Importing untrained model
2025-10-27 02:16:42,952:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:16:42,957:INFO:Starting cross validation
2025-10-27 02:16:42,958:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:16:55,767:INFO:Calculating mean and std
2025-10-27 02:16:55,768:INFO:Creating metrics dataframe
2025-10-27 02:16:55,771:INFO:Finalizing model
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000310 seconds.
2025-10-27 02:16:55,804:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:16:55,804:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Total Bins 836
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 14
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:16:55,804:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:16:55,805:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:16:55,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:55,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:56,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:16:57,068:INFO:Uploading results into container
2025-10-27 02:16:57,069:INFO:Uploading model into container now
2025-10-27 02:16:57,076:INFO:_master_model_container: 15
2025-10-27 02:16:57,076:INFO:_display_container: 4
2025-10-27 02:16:57,076:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:16:57,077:INFO:create_model() successfully completed......................................
2025-10-27 02:16:57,192:INFO:Soft dependency imported: gradio: 5.49.1
2025-10-27 02:19:43,981:INFO:Initializing predict_model()
2025-10-27 02:19:43,981:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1bcca682c0>)
2025-10-27 02:19:43,981:INFO:Checking exceptions
2025-10-27 02:19:43,981:INFO:Preloading libraries
2025-10-27 02:19:43,982:INFO:Set up data.
2025-10-27 02:19:43,985:INFO:Set up index.
2025-10-27 02:22:50,215:INFO:Initializing predict_model()
2025-10-27 02:22:50,215:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1bce1b1f90>, estimator=LGBMClassifier(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1bcec3aca0>)
2025-10-27 02:22:50,215:INFO:Checking exceptions
2025-10-27 02:22:50,215:INFO:Preloading libraries
2025-10-27 02:22:50,217:INFO:Set up data.
2025-10-27 02:22:50,221:INFO:Set up index.
2025-10-27 02:24:10,043:INFO:PyCaret ClassificationExperiment
2025-10-27 02:24:10,043:INFO:Logging name: clf-default-name
2025-10-27 02:24:10,043:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-27 02:24:10,043:INFO:version 3.3.2
2025-10-27 02:24:10,043:INFO:Initializing setup()
2025-10-27 02:24:10,043:INFO:self.USI: 2705
2025-10-27 02:24:10,043:INFO:self._variable_keys: {'fix_imbalance', 'fold_generator', 'X', 'idx', 'target_param', '_ml_usecase', 'exp_name_log', 'pipeline', 'memory', 'html_param', 'X_train', 'fold_groups_param', 'y_train', 'gpu_n_jobs_param', 'gpu_param', 'fold_shuffle_param', 'y', 'data', '_available_plots', 'log_plots_param', 'X_test', 'seed', 'USI', 'exp_id', 'logging_param', 'y_test', 'is_multiclass', 'n_jobs_param'}
2025-10-27 02:24:10,043:INFO:Checking environment
2025-10-27 02:24:10,043:INFO:python_version: 3.11.14
2025-10-27 02:24:10,043:INFO:python_build: ('main', 'Oct 24 2025 09:18:30')
2025-10-27 02:24:10,043:INFO:machine: x86_64
2025-10-27 02:24:10,043:INFO:platform: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:24:10,043:INFO:Memory: svmem(total=16365076480, available=7652597760, percent=53.2, used=8712478720, free=3362545664, active=3185741824, inactive=7384563712, buffers=24576, cached=6314590208, shared=1360613376, slab=1308856320)
2025-10-27 02:24:10,044:INFO:Physical Core: 12
2025-10-27 02:24:10,044:INFO:Logical Core: 16
2025-10-27 02:24:10,044:INFO:Checking libraries
2025-10-27 02:24:10,044:INFO:System:
2025-10-27 02:24:10,044:INFO:    python: 3.11.14 (main, Oct 24 2025, 09:18:30) [GCC 15.2.1 20250813]
2025-10-27 02:24:10,044:INFO:executable: /home/ykalathiya/pycaret_assignment/.venv/bin/python
2025-10-27 02:24:10,044:INFO:   machine: Linux-6.16.6-arch1-1-x86_64-with-glibc2.42
2025-10-27 02:24:10,044:INFO:PyCaret required dependencies:
2025-10-27 02:24:10,044:INFO:                 pip: 25.2
2025-10-27 02:24:10,045:INFO:          setuptools: 80.9.0
2025-10-27 02:24:10,045:INFO:             pycaret: 3.3.2
2025-10-27 02:24:10,045:INFO:             IPython: 9.6.0
2025-10-27 02:24:10,045:INFO:          ipywidgets: 8.1.7
2025-10-27 02:24:10,045:INFO:                tqdm: 4.67.1
2025-10-27 02:24:10,045:INFO:               numpy: 1.26.4
2025-10-27 02:24:10,045:INFO:              pandas: 2.1.4
2025-10-27 02:24:10,045:INFO:              jinja2: 3.1.6
2025-10-27 02:24:10,045:INFO:               scipy: 1.11.4
2025-10-27 02:24:10,045:INFO:              joblib: 1.3.2
2025-10-27 02:24:10,045:INFO:             sklearn: 1.4.2
2025-10-27 02:24:10,045:INFO:                pyod: 2.0.5
2025-10-27 02:24:10,045:INFO:            imblearn: 0.14.0
2025-10-27 02:24:10,045:INFO:   category_encoders: 2.7.0
2025-10-27 02:24:10,045:INFO:            lightgbm: 4.6.0
2025-10-27 02:24:10,045:INFO:               numba: 0.61.0
2025-10-27 02:24:10,045:INFO:            requests: 2.32.5
2025-10-27 02:24:10,045:INFO:          matplotlib: 3.7.5
2025-10-27 02:24:10,045:INFO:          scikitplot: 0.3.7
2025-10-27 02:24:10,045:INFO:         yellowbrick: 1.5
2025-10-27 02:24:10,045:INFO:              plotly: 5.24.1
2025-10-27 02:24:10,045:INFO:    plotly-resampler: Not installed
2025-10-27 02:24:10,045:INFO:             kaleido: 1.1.0
2025-10-27 02:24:10,045:INFO:           schemdraw: 0.15
2025-10-27 02:24:10,045:INFO:         statsmodels: 0.14.5
2025-10-27 02:24:10,045:INFO:              sktime: 0.26.0
2025-10-27 02:24:10,045:INFO:               tbats: 1.1.3
2025-10-27 02:24:10,045:INFO:            pmdarima: 2.0.4
2025-10-27 02:24:10,045:INFO:              psutil: 7.1.1
2025-10-27 02:24:10,045:INFO:          markupsafe: 3.0.3
2025-10-27 02:24:10,045:INFO:             pickle5: Not installed
2025-10-27 02:24:10,046:INFO:         cloudpickle: 3.1.1
2025-10-27 02:24:10,046:INFO:         deprecation: 2.1.0
2025-10-27 02:24:10,046:INFO:              xxhash: 3.6.0
2025-10-27 02:24:10,046:INFO:           wurlitzer: 3.1.1
2025-10-27 02:24:10,046:INFO:PyCaret optional dependencies:
2025-10-27 02:24:10,046:INFO:                shap: 0.44.1
2025-10-27 02:24:10,046:INFO:           interpret: 0.7.3
2025-10-27 02:24:10,046:INFO:                umap: 0.5.7
2025-10-27 02:24:10,046:INFO:     ydata_profiling: 4.17.0
2025-10-27 02:24:10,046:INFO:  explainerdashboard: 0.5.1
2025-10-27 02:24:10,046:INFO:             autoviz: Not installed
2025-10-27 02:24:10,046:INFO:           fairlearn: 0.7.0
2025-10-27 02:24:10,046:INFO:          deepchecks: Not installed
2025-10-27 02:24:10,046:INFO:             xgboost: Not installed
2025-10-27 02:24:10,046:INFO:            catboost: Not installed
2025-10-27 02:24:10,046:INFO:              kmodes: Not installed
2025-10-27 02:24:10,046:INFO:             mlxtend: Not installed
2025-10-27 02:24:10,046:INFO:       statsforecast: Not installed
2025-10-27 02:24:10,046:INFO:        tune_sklearn: Not installed
2025-10-27 02:24:10,046:INFO:                 ray: Not installed
2025-10-27 02:24:10,046:INFO:            hyperopt: Not installed
2025-10-27 02:24:10,046:INFO:              optuna: Not installed
2025-10-27 02:24:10,046:INFO:               skopt: Not installed
2025-10-27 02:24:10,046:INFO:              mlflow: 3.5.1
2025-10-27 02:24:10,046:INFO:              gradio: 5.49.1
2025-10-27 02:24:10,046:INFO:             fastapi: 0.120.0
2025-10-27 02:24:10,046:INFO:             uvicorn: 0.38.0
2025-10-27 02:24:10,046:INFO:              m2cgen: 0.10.0
2025-10-27 02:24:10,046:INFO:           evidently: 0.4.40
2025-10-27 02:24:10,046:INFO:               fugue: Not installed
2025-10-27 02:24:10,047:INFO:           streamlit: Not installed
2025-10-27 02:24:10,047:INFO:             prophet: Not installed
2025-10-27 02:24:10,047:INFO:None
2025-10-27 02:24:10,047:INFO:Set up data.
2025-10-27 02:24:10,050:INFO:Set up folding strategy.
2025-10-27 02:24:10,050:INFO:Set up train/test split.
2025-10-27 02:24:10,055:INFO:Set up index.
2025-10-27 02:24:10,055:INFO:Assigning column types.
2025-10-27 02:24:10,057:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-27 02:24:10,091:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,104:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,105:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,134:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,135:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,147:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,147:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,147:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-27 02:24:10,170:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,184:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,184:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,206:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-27 02:24:10,220:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,220:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,220:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-27 02:24:10,256:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,256:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,292:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,292:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,293:INFO:Preparing preprocessing pipeline...
2025-10-27 02:24:10,294:INFO:Set up simple imputation.
2025-10-27 02:24:10,294:INFO:Set up column transformation.
2025-10-27 02:24:10,294:INFO:Set up feature normalization.
2025-10-27 02:24:10,333:INFO:Finished creating preprocessing pipeline.
2025-10-27 02:24:10,336:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=Non...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-27 02:24:10,336:INFO:Creating final display dataframe.
2025-10-27 02:24:10,389:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 14)
4        Transformed data shape        (2392, 14)
5   Transformed train set shape        (1674, 14)
6    Transformed test set shape         (718, 14)
7              Numeric features                13
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              2705
2025-10-27 02:24:10,427:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,427:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,462:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-27 02:24:10,463:INFO:setup() successfully completed in 0.42s...............
2025-10-27 02:24:10,476:INFO:Initializing compare_models()
2025-10-27 02:24:10,476:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-10-27 02:24:10,476:INFO:Checking exceptions
2025-10-27 02:24:10,480:INFO:Preparing display monitor
2025-10-27 02:24:10,492:INFO:Initializing Logistic Regression
2025-10-27 02:24:10,492:INFO:Total runtime is 1.7642974853515626e-06 minutes
2025-10-27 02:24:10,494:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:10,494:INFO:Initializing create_model()
2025-10-27 02:24:10,494:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:10,494:INFO:Checking exceptions
2025-10-27 02:24:10,494:INFO:Importing libraries
2025-10-27 02:24:10,494:INFO:Copying training dataset
2025-10-27 02:24:10,497:INFO:Defining folds
2025-10-27 02:24:10,497:INFO:Declaring metric variables
2025-10-27 02:24:10,499:INFO:Importing untrained model
2025-10-27 02:24:10,501:INFO:Logistic Regression Imported successfully
2025-10-27 02:24:10,503:INFO:Starting cross validation
2025-10-27 02:24:10,504:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:12,272:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,342:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,382:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,403:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,481:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,537:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,556:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,558:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,561:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:12,595:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,597:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:12,611:INFO:Calculating mean and std
2025-10-27 02:24:12,612:INFO:Creating metrics dataframe
2025-10-27 02:24:12,614:INFO:Uploading results into container
2025-10-27 02:24:12,615:INFO:Uploading model into container now
2025-10-27 02:24:12,615:INFO:_master_model_container: 1
2025-10-27 02:24:12,615:INFO:_display_container: 2
2025-10-27 02:24:12,616:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-27 02:24:12,616:INFO:create_model() successfully completed......................................
2025-10-27 02:24:12,785:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:12,786:INFO:Creating metrics dataframe
2025-10-27 02:24:12,789:INFO:Initializing K Neighbors Classifier
2025-10-27 02:24:12,789:INFO:Total runtime is 0.038293528556823726 minutes
2025-10-27 02:24:12,791:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:12,791:INFO:Initializing create_model()
2025-10-27 02:24:12,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:12,791:INFO:Checking exceptions
2025-10-27 02:24:12,791:INFO:Importing libraries
2025-10-27 02:24:12,791:INFO:Copying training dataset
2025-10-27 02:24:12,794:INFO:Defining folds
2025-10-27 02:24:12,795:INFO:Declaring metric variables
2025-10-27 02:24:12,797:INFO:Importing untrained model
2025-10-27 02:24:12,798:INFO:K Neighbors Classifier Imported successfully
2025-10-27 02:24:12,801:INFO:Starting cross validation
2025-10-27 02:24:12,802:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:14,404:INFO:Calculating mean and std
2025-10-27 02:24:14,404:INFO:Creating metrics dataframe
2025-10-27 02:24:14,405:INFO:Uploading results into container
2025-10-27 02:24:14,406:INFO:Uploading model into container now
2025-10-27 02:24:14,406:INFO:_master_model_container: 2
2025-10-27 02:24:14,406:INFO:_display_container: 2
2025-10-27 02:24:14,406:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-27 02:24:14,406:INFO:create_model() successfully completed......................................
2025-10-27 02:24:14,556:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:14,557:INFO:Creating metrics dataframe
2025-10-27 02:24:14,561:INFO:Initializing Naive Bayes
2025-10-27 02:24:14,561:INFO:Total runtime is 0.06781365871429443 minutes
2025-10-27 02:24:14,562:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:14,563:INFO:Initializing create_model()
2025-10-27 02:24:14,563:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:14,563:INFO:Checking exceptions
2025-10-27 02:24:14,563:INFO:Importing libraries
2025-10-27 02:24:14,563:INFO:Copying training dataset
2025-10-27 02:24:14,566:INFO:Defining folds
2025-10-27 02:24:14,566:INFO:Declaring metric variables
2025-10-27 02:24:14,568:INFO:Importing untrained model
2025-10-27 02:24:14,569:INFO:Naive Bayes Imported successfully
2025-10-27 02:24:14,572:INFO:Starting cross validation
2025-10-27 02:24:14,573:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:14,624:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,625:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,627:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,630:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,656:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,657:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,671:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:14,680:INFO:Calculating mean and std
2025-10-27 02:24:14,681:INFO:Creating metrics dataframe
2025-10-27 02:24:14,682:INFO:Uploading results into container
2025-10-27 02:24:14,682:INFO:Uploading model into container now
2025-10-27 02:24:14,682:INFO:_master_model_container: 3
2025-10-27 02:24:14,682:INFO:_display_container: 2
2025-10-27 02:24:14,682:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-27 02:24:14,682:INFO:create_model() successfully completed......................................
2025-10-27 02:24:14,831:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:14,831:INFO:Creating metrics dataframe
2025-10-27 02:24:14,835:INFO:Initializing Decision Tree Classifier
2025-10-27 02:24:14,835:INFO:Total runtime is 0.07239251931508382 minutes
2025-10-27 02:24:14,837:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:14,837:INFO:Initializing create_model()
2025-10-27 02:24:14,837:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:14,837:INFO:Checking exceptions
2025-10-27 02:24:14,837:INFO:Importing libraries
2025-10-27 02:24:14,837:INFO:Copying training dataset
2025-10-27 02:24:14,840:INFO:Defining folds
2025-10-27 02:24:14,840:INFO:Declaring metric variables
2025-10-27 02:24:14,842:INFO:Importing untrained model
2025-10-27 02:24:14,844:INFO:Decision Tree Classifier Imported successfully
2025-10-27 02:24:14,849:INFO:Starting cross validation
2025-10-27 02:24:14,850:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:14,984:INFO:Calculating mean and std
2025-10-27 02:24:14,984:INFO:Creating metrics dataframe
2025-10-27 02:24:14,985:INFO:Uploading results into container
2025-10-27 02:24:14,986:INFO:Uploading model into container now
2025-10-27 02:24:14,986:INFO:_master_model_container: 4
2025-10-27 02:24:14,986:INFO:_display_container: 2
2025-10-27 02:24:14,986:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-10-27 02:24:14,986:INFO:create_model() successfully completed......................................
2025-10-27 02:24:15,151:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:15,151:INFO:Creating metrics dataframe
2025-10-27 02:24:15,155:INFO:Initializing SVM - Linear Kernel
2025-10-27 02:24:15,156:INFO:Total runtime is 0.07772899468739827 minutes
2025-10-27 02:24:15,157:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:15,158:INFO:Initializing create_model()
2025-10-27 02:24:15,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:15,158:INFO:Checking exceptions
2025-10-27 02:24:15,158:INFO:Importing libraries
2025-10-27 02:24:15,158:INFO:Copying training dataset
2025-10-27 02:24:15,161:INFO:Defining folds
2025-10-27 02:24:15,161:INFO:Declaring metric variables
2025-10-27 02:24:15,163:INFO:Importing untrained model
2025-10-27 02:24:15,165:INFO:SVM - Linear Kernel Imported successfully
2025-10-27 02:24:15,170:INFO:Starting cross validation
2025-10-27 02:24:15,171:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:15,246:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,255:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,273:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,274:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,276:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,290:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,307:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,308:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,311:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,317:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,331:INFO:Calculating mean and std
2025-10-27 02:24:15,332:INFO:Creating metrics dataframe
2025-10-27 02:24:15,333:INFO:Uploading results into container
2025-10-27 02:24:15,333:INFO:Uploading model into container now
2025-10-27 02:24:15,334:INFO:_master_model_container: 5
2025-10-27 02:24:15,334:INFO:_display_container: 2
2025-10-27 02:24:15,334:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-27 02:24:15,334:INFO:create_model() successfully completed......................................
2025-10-27 02:24:15,491:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:15,491:INFO:Creating metrics dataframe
2025-10-27 02:24:15,495:INFO:Initializing Ridge Classifier
2025-10-27 02:24:15,495:INFO:Total runtime is 0.08338807821273804 minutes
2025-10-27 02:24:15,497:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:15,497:INFO:Initializing create_model()
2025-10-27 02:24:15,497:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:15,497:INFO:Checking exceptions
2025-10-27 02:24:15,497:INFO:Importing libraries
2025-10-27 02:24:15,497:INFO:Copying training dataset
2025-10-27 02:24:15,500:INFO:Defining folds
2025-10-27 02:24:15,500:INFO:Declaring metric variables
2025-10-27 02:24:15,502:INFO:Importing untrained model
2025-10-27 02:24:15,503:INFO:Ridge Classifier Imported successfully
2025-10-27 02:24:15,506:INFO:Starting cross validation
2025-10-27 02:24:15,507:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:15,557:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,558:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,560:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,561:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,563:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,566:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,569:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,572:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,587:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,589:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,589:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,589:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,591:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,592:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,592:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,594:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,603:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,605:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:15,605:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,607:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:15,614:INFO:Calculating mean and std
2025-10-27 02:24:15,615:INFO:Creating metrics dataframe
2025-10-27 02:24:15,616:INFO:Uploading results into container
2025-10-27 02:24:15,616:INFO:Uploading model into container now
2025-10-27 02:24:15,616:INFO:_master_model_container: 6
2025-10-27 02:24:15,616:INFO:_display_container: 2
2025-10-27 02:24:15,616:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-10-27 02:24:15,616:INFO:create_model() successfully completed......................................
2025-10-27 02:24:15,760:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:15,760:INFO:Creating metrics dataframe
2025-10-27 02:24:15,765:INFO:Initializing Random Forest Classifier
2025-10-27 02:24:15,765:INFO:Total runtime is 0.08788096904754639 minutes
2025-10-27 02:24:15,766:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:15,766:INFO:Initializing create_model()
2025-10-27 02:24:15,766:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:15,766:INFO:Checking exceptions
2025-10-27 02:24:15,766:INFO:Importing libraries
2025-10-27 02:24:15,766:INFO:Copying training dataset
2025-10-27 02:24:15,769:INFO:Defining folds
2025-10-27 02:24:15,769:INFO:Declaring metric variables
2025-10-27 02:24:15,771:INFO:Importing untrained model
2025-10-27 02:24:15,772:INFO:Random Forest Classifier Imported successfully
2025-10-27 02:24:15,775:INFO:Starting cross validation
2025-10-27 02:24:15,775:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:16,190:INFO:Calculating mean and std
2025-10-27 02:24:16,191:INFO:Creating metrics dataframe
2025-10-27 02:24:16,192:INFO:Uploading results into container
2025-10-27 02:24:16,193:INFO:Uploading model into container now
2025-10-27 02:24:16,193:INFO:_master_model_container: 7
2025-10-27 02:24:16,193:INFO:_display_container: 2
2025-10-27 02:24:16,193:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-10-27 02:24:16,193:INFO:create_model() successfully completed......................................
2025-10-27 02:24:16,342:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:16,342:INFO:Creating metrics dataframe
2025-10-27 02:24:16,348:INFO:Initializing Quadratic Discriminant Analysis
2025-10-27 02:24:16,348:INFO:Total runtime is 0.09759661356608074 minutes
2025-10-27 02:24:16,349:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:16,349:INFO:Initializing create_model()
2025-10-27 02:24:16,349:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:16,350:INFO:Checking exceptions
2025-10-27 02:24:16,350:INFO:Importing libraries
2025-10-27 02:24:16,350:INFO:Copying training dataset
2025-10-27 02:24:16,352:INFO:Defining folds
2025-10-27 02:24:16,352:INFO:Declaring metric variables
2025-10-27 02:24:16,354:INFO:Importing untrained model
2025-10-27 02:24:16,355:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-27 02:24:16,358:INFO:Starting cross validation
2025-10-27 02:24:16,358:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:16,410:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,410:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,412:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,418:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,441:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,444:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,445:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,447:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,450:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,455:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,466:INFO:Calculating mean and std
2025-10-27 02:24:16,466:INFO:Creating metrics dataframe
2025-10-27 02:24:16,468:INFO:Uploading results into container
2025-10-27 02:24:16,468:INFO:Uploading model into container now
2025-10-27 02:24:16,468:INFO:_master_model_container: 8
2025-10-27 02:24:16,468:INFO:_display_container: 2
2025-10-27 02:24:16,468:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-27 02:24:16,468:INFO:create_model() successfully completed......................................
2025-10-27 02:24:16,610:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:16,610:INFO:Creating metrics dataframe
2025-10-27 02:24:16,615:INFO:Initializing Ada Boost Classifier
2025-10-27 02:24:16,615:INFO:Total runtime is 0.10204780896504721 minutes
2025-10-27 02:24:16,616:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:16,616:INFO:Initializing create_model()
2025-10-27 02:24:16,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:16,616:INFO:Checking exceptions
2025-10-27 02:24:16,616:INFO:Importing libraries
2025-10-27 02:24:16,616:INFO:Copying training dataset
2025-10-27 02:24:16,619:INFO:Defining folds
2025-10-27 02:24:16,619:INFO:Declaring metric variables
2025-10-27 02:24:16,620:INFO:Importing untrained model
2025-10-27 02:24:16,622:INFO:Ada Boost Classifier Imported successfully
2025-10-27 02:24:16,625:INFO:Starting cross validation
2025-10-27 02:24:16,626:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:16,675:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,679:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,684:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,690:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,714:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,719:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,722:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,722:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,723:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,729:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-27 02:24:16,770:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,774:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,780:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,788:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,837:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,837:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,840:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,858:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,860:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,867:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:16,875:INFO:Calculating mean and std
2025-10-27 02:24:16,876:INFO:Creating metrics dataframe
2025-10-27 02:24:16,877:INFO:Uploading results into container
2025-10-27 02:24:16,877:INFO:Uploading model into container now
2025-10-27 02:24:16,877:INFO:_master_model_container: 9
2025-10-27 02:24:16,877:INFO:_display_container: 2
2025-10-27 02:24:16,878:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-10-27 02:24:16,878:INFO:create_model() successfully completed......................................
2025-10-27 02:24:17,022:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:17,022:INFO:Creating metrics dataframe
2025-10-27 02:24:17,026:INFO:Initializing Gradient Boosting Classifier
2025-10-27 02:24:17,026:INFO:Total runtime is 0.10891013542811077 minutes
2025-10-27 02:24:17,028:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:17,028:INFO:Initializing create_model()
2025-10-27 02:24:17,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:17,028:INFO:Checking exceptions
2025-10-27 02:24:17,028:INFO:Importing libraries
2025-10-27 02:24:17,028:INFO:Copying training dataset
2025-10-27 02:24:17,031:INFO:Defining folds
2025-10-27 02:24:17,031:INFO:Declaring metric variables
2025-10-27 02:24:17,033:INFO:Importing untrained model
2025-10-27 02:24:17,034:INFO:Gradient Boosting Classifier Imported successfully
2025-10-27 02:24:17,037:INFO:Starting cross validation
2025-10-27 02:24:17,038:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:18,169:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,171:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,215:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,268:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,549:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,565:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,580:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,584:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,688:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,703:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,712:INFO:Calculating mean and std
2025-10-27 02:24:18,713:INFO:Creating metrics dataframe
2025-10-27 02:24:18,714:INFO:Uploading results into container
2025-10-27 02:24:18,715:INFO:Uploading model into container now
2025-10-27 02:24:18,715:INFO:_master_model_container: 10
2025-10-27 02:24:18,715:INFO:_display_container: 2
2025-10-27 02:24:18,715:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-27 02:24:18,715:INFO:create_model() successfully completed......................................
2025-10-27 02:24:18,856:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:18,856:INFO:Creating metrics dataframe
2025-10-27 02:24:18,860:INFO:Initializing Linear Discriminant Analysis
2025-10-27 02:24:18,860:INFO:Total runtime is 0.13947269519170127 minutes
2025-10-27 02:24:18,863:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:18,863:INFO:Initializing create_model()
2025-10-27 02:24:18,863:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:18,863:INFO:Checking exceptions
2025-10-27 02:24:18,863:INFO:Importing libraries
2025-10-27 02:24:18,863:INFO:Copying training dataset
2025-10-27 02:24:18,866:INFO:Defining folds
2025-10-27 02:24:18,866:INFO:Declaring metric variables
2025-10-27 02:24:18,868:INFO:Importing untrained model
2025-10-27 02:24:18,870:INFO:Linear Discriminant Analysis Imported successfully
2025-10-27 02:24:18,873:INFO:Starting cross validation
2025-10-27 02:24:18,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:18,925:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,926:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,929:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:18,932:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,934:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:18,935:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,940:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,954:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,956:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,966:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,974:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,978:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-27 02:24:18,991:INFO:Calculating mean and std
2025-10-27 02:24:18,991:INFO:Creating metrics dataframe
2025-10-27 02:24:18,992:INFO:Uploading results into container
2025-10-27 02:24:18,993:INFO:Uploading model into container now
2025-10-27 02:24:18,993:INFO:_master_model_container: 11
2025-10-27 02:24:18,993:INFO:_display_container: 2
2025-10-27 02:24:18,993:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-27 02:24:18,993:INFO:create_model() successfully completed......................................
2025-10-27 02:24:19,135:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:19,135:INFO:Creating metrics dataframe
2025-10-27 02:24:19,140:INFO:Initializing Extra Trees Classifier
2025-10-27 02:24:19,140:INFO:Total runtime is 0.1441314180692037 minutes
2025-10-27 02:24:19,141:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:19,141:INFO:Initializing create_model()
2025-10-27 02:24:19,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:19,141:INFO:Checking exceptions
2025-10-27 02:24:19,141:INFO:Importing libraries
2025-10-27 02:24:19,141:INFO:Copying training dataset
2025-10-27 02:24:19,145:INFO:Defining folds
2025-10-27 02:24:19,145:INFO:Declaring metric variables
2025-10-27 02:24:19,147:INFO:Importing untrained model
2025-10-27 02:24:19,149:INFO:Extra Trees Classifier Imported successfully
2025-10-27 02:24:19,152:INFO:Starting cross validation
2025-10-27 02:24:19,152:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:19,524:INFO:Calculating mean and std
2025-10-27 02:24:19,524:INFO:Creating metrics dataframe
2025-10-27 02:24:19,525:INFO:Uploading results into container
2025-10-27 02:24:19,525:INFO:Uploading model into container now
2025-10-27 02:24:19,526:INFO:_master_model_container: 12
2025-10-27 02:24:19,526:INFO:_display_container: 2
2025-10-27 02:24:19,526:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-10-27 02:24:19,526:INFO:create_model() successfully completed......................................
2025-10-27 02:24:19,668:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:19,668:INFO:Creating metrics dataframe
2025-10-27 02:24:19,673:INFO:Initializing Light Gradient Boosting Machine
2025-10-27 02:24:19,673:INFO:Total runtime is 0.15301371812820436 minutes
2025-10-27 02:24:19,674:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:19,674:INFO:Initializing create_model()
2025-10-27 02:24:19,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:19,674:INFO:Checking exceptions
2025-10-27 02:24:19,675:INFO:Importing libraries
2025-10-27 02:24:19,675:INFO:Copying training dataset
2025-10-27 02:24:19,677:INFO:Defining folds
2025-10-27 02:24:19,677:INFO:Declaring metric variables
2025-10-27 02:24:19,680:INFO:Importing untrained model
2025-10-27 02:24:19,682:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:24:19,685:INFO:Starting cross validation
2025-10-27 02:24:19,685:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:31,551:INFO:Calculating mean and std
2025-10-27 02:24:31,552:INFO:Creating metrics dataframe
2025-10-27 02:24:31,553:INFO:Uploading results into container
2025-10-27 02:24:31,554:INFO:Uploading model into container now
2025-10-27 02:24:31,554:INFO:_master_model_container: 13
2025-10-27 02:24:31,554:INFO:_display_container: 2
2025-10-27 02:24:31,554:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:24:31,554:INFO:create_model() successfully completed......................................
2025-10-27 02:24:31,698:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:31,698:INFO:Creating metrics dataframe
2025-10-27 02:24:31,703:INFO:Initializing Dummy Classifier
2025-10-27 02:24:31,704:INFO:Total runtime is 0.35353044668833417 minutes
2025-10-27 02:24:31,705:INFO:SubProcess create_model() called ==================================
2025-10-27 02:24:31,705:INFO:Initializing create_model()
2025-10-27 02:24:31,705:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f1acf516bd0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:31,705:INFO:Checking exceptions
2025-10-27 02:24:31,705:INFO:Importing libraries
2025-10-27 02:24:31,706:INFO:Copying training dataset
2025-10-27 02:24:31,708:INFO:Defining folds
2025-10-27 02:24:31,708:INFO:Declaring metric variables
2025-10-27 02:24:31,710:INFO:Importing untrained model
2025-10-27 02:24:31,713:INFO:Dummy Classifier Imported successfully
2025-10-27 02:24:31,717:INFO:Starting cross validation
2025-10-27 02:24:31,717:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:31,774:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,776:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,778:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,779:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,805:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,808:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,816:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,819:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,827:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,832:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-27 02:24:31,846:INFO:Calculating mean and std
2025-10-27 02:24:31,846:INFO:Creating metrics dataframe
2025-10-27 02:24:31,847:INFO:Uploading results into container
2025-10-27 02:24:31,848:INFO:Uploading model into container now
2025-10-27 02:24:31,848:INFO:_master_model_container: 14
2025-10-27 02:24:31,848:INFO:_display_container: 2
2025-10-27 02:24:31,848:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-10-27 02:24:31,848:INFO:create_model() successfully completed......................................
2025-10-27 02:24:31,990:INFO:SubProcess create_model() end ==================================
2025-10-27 02:24:31,990:INFO:Creating metrics dataframe
2025-10-27 02:24:31,996:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2025-10-27 02:24:32,001:INFO:Initializing create_model()
2025-10-27 02:24:32,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:32,002:INFO:Checking exceptions
2025-10-27 02:24:32,003:INFO:Importing libraries
2025-10-27 02:24:32,003:INFO:Copying training dataset
2025-10-27 02:24:32,005:INFO:Defining folds
2025-10-27 02:24:32,005:INFO:Declaring metric variables
2025-10-27 02:24:32,005:INFO:Importing untrained model
2025-10-27 02:24:32,005:INFO:Declaring custom model
2025-10-27 02:24:32,006:INFO:Gradient Boosting Classifier Imported successfully
2025-10-27 02:24:32,006:INFO:Cross validation set to False
2025-10-27 02:24:32,006:INFO:Fitting Model
2025-10-27 02:24:33,071:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-27 02:24:33,071:INFO:create_model() successfully completed......................................
2025-10-27 02:24:33,226:INFO:_master_model_container: 14
2025-10-27 02:24:33,226:INFO:_display_container: 2
2025-10-27 02:24:33,226:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-27 02:24:33,226:INFO:compare_models() successfully completed......................................
2025-10-27 02:24:33,244:INFO:Initializing evaluate_model()
2025-10-27 02:24:33,245:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-10-27 02:24:33,250:INFO:Initializing plot_model()
2025-10-27 02:24:33,250:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-10-27 02:24:33,250:INFO:Checking exceptions
2025-10-27 02:24:33,253:INFO:Preloading libraries
2025-10-27 02:24:33,270:INFO:Copying training dataset
2025-10-27 02:24:33,270:INFO:Plot type: pipeline
2025-10-27 02:24:33,353:INFO:Visual Rendered Successfully
2025-10-27 02:24:33,508:INFO:plot_model() successfully completed......................................
2025-10-27 02:24:33,527:INFO:Initializing save_model()
2025-10-27 02:24:33,527:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=best_student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=Non...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-10-27 02:24:33,527:INFO:Adding model into prep_pipe
2025-10-27 02:24:33,557:INFO:best_student_performance_model.pkl saved in current working directory
2025-10-27 02:24:33,562:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empt...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=123, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-10-27 02:24:33,563:INFO:save_model() successfully completed......................................
2025-10-27 02:24:33,734:INFO:Initializing predict_model()
2025-10-27 02:24:33,734:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1acf2e71a0>)
2025-10-27 02:24:33,734:INFO:Checking exceptions
2025-10-27 02:24:33,734:INFO:Preloading libraries
2025-10-27 02:24:33,949:INFO:Initializing create_model()
2025-10-27 02:24:33,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-27 02:24:33,949:INFO:Checking exceptions
2025-10-27 02:24:33,957:INFO:Importing libraries
2025-10-27 02:24:33,957:INFO:Copying training dataset
2025-10-27 02:24:33,960:INFO:Defining folds
2025-10-27 02:24:33,960:INFO:Declaring metric variables
2025-10-27 02:24:33,962:INFO:Importing untrained model
2025-10-27 02:24:33,964:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-27 02:24:33,967:INFO:Starting cross validation
2025-10-27 02:24:33,968:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-27 02:24:46,615:INFO:Calculating mean and std
2025-10-27 02:24:46,615:INFO:Creating metrics dataframe
2025-10-27 02:24:46,620:INFO:Finalizing model
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.
2025-10-27 02:24:46,660:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-27 02:24:46,660:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Total Bins 581
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 13
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-27 02:24:46,660:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-27 02:24:46,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:24:46,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-27 02:24:47,992:INFO:Uploading results into container
2025-10-27 02:24:47,993:INFO:Uploading model into container now
2025-10-27 02:24:47,998:INFO:_master_model_container: 15
2025-10-27 02:24:47,998:INFO:_display_container: 4
2025-10-27 02:24:47,999:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-27 02:24:47,999:INFO:create_model() successfully completed......................................
2025-10-27 02:24:48,174:INFO:Soft dependency imported: gradio: 5.49.1
2025-10-27 02:26:28,984:INFO:Initializing predict_model()
2025-10-27 02:26:28,984:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=LGBMClassifier(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1acf0fa980>)
2025-10-27 02:26:28,984:INFO:Checking exceptions
2025-10-27 02:26:28,984:INFO:Preloading libraries
2025-10-27 02:26:28,985:INFO:Set up data.
2025-10-27 02:26:28,988:INFO:Set up index.
2025-10-27 02:26:48,423:INFO:Initializing predict_model()
2025-10-27 02:26:48,423:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f1b5b193910>, estimator=LGBMClassifier(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f1acf3b4d60>)
2025-10-27 02:26:48,423:INFO:Checking exceptions
2025-10-27 02:26:48,423:INFO:Preloading libraries
2025-10-27 02:26:48,424:INFO:Set up data.
2025-10-27 02:26:48,426:INFO:Set up index.
2025-10-28 00:09:22,327:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-28 00:09:22,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-28 00:09:22,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-28 00:09:22,328:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-10-28 00:09:22,854:INFO:PyCaret ClassificationExperiment
2025-10-28 00:09:22,854:INFO:Logging name: clf-default-name
2025-10-28 00:09:22,854:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-10-28 00:09:22,854:INFO:version 3.3.2
2025-10-28 00:09:22,854:INFO:Initializing setup()
2025-10-28 00:09:22,854:INFO:self.USI: c125
2025-10-28 00:09:22,854:INFO:self._variable_keys: {'_ml_usecase', 'fix_imbalance', 'n_jobs_param', 'exp_name_log', 'X_train', 'X', 'target_param', 'gpu_param', 'X_test', 'memory', 'log_plots_param', 'gpu_n_jobs_param', 'html_param', 'data', 'y_train', 'fold_generator', 'is_multiclass', 'USI', 'exp_id', 'y_test', '_available_plots', 'fold_groups_param', 'seed', 'idx', 'y', 'pipeline', 'fold_shuffle_param', 'logging_param'}
2025-10-28 00:09:22,854:INFO:Checking environment
2025-10-28 00:09:22,854:INFO:python_version: 3.11.14
2025-10-28 00:09:22,854:INFO:python_build: ('main', 'Oct 24 2025 09:18:30')
2025-10-28 00:09:22,854:INFO:machine: x86_64
2025-10-28 00:09:22,854:INFO:platform: Linux-6.17.5-arch1-1-x86_64-with-glibc2.42
2025-10-28 00:09:22,854:INFO:Memory: svmem(total=16364691456, available=7325396992, percent=55.2, used=9039294464, free=2300985344, active=7819464704, inactive=3105837056, buffers=3035136, cached=8097501184, shared=2723225600, slab=718770176)
2025-10-28 00:09:22,855:INFO:Physical Core: 12
2025-10-28 00:09:22,855:INFO:Logical Core: 16
2025-10-28 00:09:22,855:INFO:Checking libraries
2025-10-28 00:09:22,855:INFO:System:
2025-10-28 00:09:22,855:INFO:    python: 3.11.14 (main, Oct 24 2025, 09:18:30) [GCC 15.2.1 20250813]
2025-10-28 00:09:22,855:INFO:executable: /home/ykalathiya/pycaret_assignment/.venv/bin/python
2025-10-28 00:09:22,855:INFO:   machine: Linux-6.17.5-arch1-1-x86_64-with-glibc2.42
2025-10-28 00:09:22,855:INFO:PyCaret required dependencies:
2025-10-28 00:09:22,969:INFO:                 pip: 25.2
2025-10-28 00:09:22,969:INFO:          setuptools: 80.9.0
2025-10-28 00:09:22,969:INFO:             pycaret: 3.3.2
2025-10-28 00:09:22,969:INFO:             IPython: 9.6.0
2025-10-28 00:09:22,969:INFO:          ipywidgets: 8.1.7
2025-10-28 00:09:22,969:INFO:                tqdm: 4.67.1
2025-10-28 00:09:22,969:INFO:               numpy: 1.26.4
2025-10-28 00:09:22,969:INFO:              pandas: 2.1.4
2025-10-28 00:09:22,969:INFO:              jinja2: 3.1.6
2025-10-28 00:09:22,969:INFO:               scipy: 1.11.4
2025-10-28 00:09:22,969:INFO:              joblib: 1.3.2
2025-10-28 00:09:22,969:INFO:             sklearn: 1.4.2
2025-10-28 00:09:22,969:INFO:                pyod: 2.0.5
2025-10-28 00:09:22,969:INFO:            imblearn: 0.14.0
2025-10-28 00:09:22,969:INFO:   category_encoders: 2.7.0
2025-10-28 00:09:22,969:INFO:            lightgbm: 4.6.0
2025-10-28 00:09:22,969:INFO:               numba: 0.61.0
2025-10-28 00:09:22,969:INFO:            requests: 2.32.5
2025-10-28 00:09:22,969:INFO:          matplotlib: 3.7.5
2025-10-28 00:09:22,969:INFO:          scikitplot: 0.3.7
2025-10-28 00:09:22,969:INFO:         yellowbrick: 1.5
2025-10-28 00:09:22,969:INFO:              plotly: 5.24.1
2025-10-28 00:09:22,969:INFO:    plotly-resampler: Not installed
2025-10-28 00:09:22,969:INFO:             kaleido: 1.1.0
2025-10-28 00:09:22,969:INFO:           schemdraw: 0.15
2025-10-28 00:09:22,969:INFO:         statsmodels: 0.14.5
2025-10-28 00:09:22,969:INFO:              sktime: 0.26.0
2025-10-28 00:09:22,969:INFO:               tbats: 1.1.3
2025-10-28 00:09:22,969:INFO:            pmdarima: 2.0.4
2025-10-28 00:09:22,969:INFO:              psutil: 7.1.1
2025-10-28 00:09:22,969:INFO:          markupsafe: 3.0.3
2025-10-28 00:09:22,969:INFO:             pickle5: Not installed
2025-10-28 00:09:22,969:INFO:         cloudpickle: 3.1.1
2025-10-28 00:09:22,969:INFO:         deprecation: 2.1.0
2025-10-28 00:09:22,969:INFO:              xxhash: 3.6.0
2025-10-28 00:09:22,969:INFO:           wurlitzer: 3.1.1
2025-10-28 00:09:22,969:INFO:PyCaret optional dependencies:
2025-10-28 00:09:24,542:INFO:                shap: 0.44.1
2025-10-28 00:09:24,542:INFO:           interpret: 0.7.3
2025-10-28 00:09:24,542:INFO:                umap: 0.5.7
2025-10-28 00:09:24,542:INFO:     ydata_profiling: 4.17.0
2025-10-28 00:09:24,542:INFO:  explainerdashboard: 0.5.1
2025-10-28 00:09:24,542:INFO:             autoviz: Not installed
2025-10-28 00:09:24,542:INFO:           fairlearn: 0.7.0
2025-10-28 00:09:24,542:INFO:          deepchecks: Not installed
2025-10-28 00:09:24,542:INFO:             xgboost: Not installed
2025-10-28 00:09:24,542:INFO:            catboost: Not installed
2025-10-28 00:09:24,542:INFO:              kmodes: Not installed
2025-10-28 00:09:24,542:INFO:             mlxtend: Not installed
2025-10-28 00:09:24,542:INFO:       statsforecast: Not installed
2025-10-28 00:09:24,542:INFO:        tune_sklearn: Not installed
2025-10-28 00:09:24,542:INFO:                 ray: Not installed
2025-10-28 00:09:24,542:INFO:            hyperopt: Not installed
2025-10-28 00:09:24,542:INFO:              optuna: Not installed
2025-10-28 00:09:24,542:INFO:               skopt: Not installed
2025-10-28 00:09:24,542:INFO:              mlflow: 3.5.1
2025-10-28 00:09:24,542:INFO:              gradio: 5.49.1
2025-10-28 00:09:24,542:INFO:             fastapi: 0.120.0
2025-10-28 00:09:24,542:INFO:             uvicorn: 0.38.0
2025-10-28 00:09:24,542:INFO:              m2cgen: 0.10.0
2025-10-28 00:09:24,542:INFO:           evidently: 0.4.40
2025-10-28 00:09:24,542:INFO:               fugue: Not installed
2025-10-28 00:09:24,542:INFO:           streamlit: Not installed
2025-10-28 00:09:24,542:INFO:             prophet: Not installed
2025-10-28 00:09:24,542:INFO:None
2025-10-28 00:09:24,542:INFO:Set up data.
2025-10-28 00:09:24,546:INFO:Set up folding strategy.
2025-10-28 00:09:24,546:INFO:Set up train/test split.
2025-10-28 00:09:24,549:INFO:Set up index.
2025-10-28 00:09:24,549:INFO:Assigning column types.
2025-10-28 00:09:24,551:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-10-28 00:09:24,574:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,578:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,598:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,598:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,621:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,621:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,636:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,636:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,636:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-10-28 00:09:24,660:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,675:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,698:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-10-28 00:09:24,713:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,713:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-10-28 00:09:24,751:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,752:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,790:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,790:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,791:INFO:Preparing preprocessing pipeline...
2025-10-28 00:09:24,792:INFO:Set up simple imputation.
2025-10-28 00:09:24,792:INFO:Set up column transformation.
2025-10-28 00:09:24,792:INFO:Set up feature normalization.
2025-10-28 00:09:24,832:INFO:Finished creating preprocessing pipeline.
2025-10-28 00:09:24,836:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=Non...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-10-28 00:09:24,836:INFO:Creating final display dataframe.
2025-10-28 00:09:24,891:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 14)
4        Transformed data shape        (2392, 14)
5   Transformed train set shape        (1674, 14)
6    Transformed test set shape         (718, 14)
7              Numeric features                13
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              c125
2025-10-28 00:09:24,933:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,933:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,972:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,972:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-10-28 00:09:24,973:INFO:setup() successfully completed in 2.12s...............
2025-10-28 00:09:24,982:INFO:Initializing compare_models()
2025-10-28 00:09:24,982:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-10-28 00:09:24,982:INFO:Checking exceptions
2025-10-28 00:09:24,986:INFO:Preparing display monitor
2025-10-28 00:09:25,000:INFO:Initializing Logistic Regression
2025-10-28 00:09:25,000:INFO:Total runtime is 2.789497375488281e-06 minutes
2025-10-28 00:09:25,003:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:25,003:INFO:Initializing create_model()
2025-10-28 00:09:25,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:25,004:INFO:Checking exceptions
2025-10-28 00:09:25,004:INFO:Importing libraries
2025-10-28 00:09:25,004:INFO:Copying training dataset
2025-10-28 00:09:25,007:INFO:Defining folds
2025-10-28 00:09:25,008:INFO:Declaring metric variables
2025-10-28 00:09:25,009:INFO:Importing untrained model
2025-10-28 00:09:25,011:INFO:Logistic Regression Imported successfully
2025-10-28 00:09:25,015:INFO:Starting cross validation
2025-10-28 00:09:25,016:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:26,991:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:26,996:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:26,997:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:27,016:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,050:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,159:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,228:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,281:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,286:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,296:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,429:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:27,436:INFO:Calculating mean and std
2025-10-28 00:09:27,437:INFO:Creating metrics dataframe
2025-10-28 00:09:27,441:INFO:Uploading results into container
2025-10-28 00:09:27,441:INFO:Uploading model into container now
2025-10-28 00:09:27,442:INFO:_master_model_container: 1
2025-10-28 00:09:27,442:INFO:_display_container: 2
2025-10-28 00:09:27,442:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-10-28 00:09:27,442:INFO:create_model() successfully completed......................................
2025-10-28 00:09:27,574:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:27,575:INFO:Creating metrics dataframe
2025-10-28 00:09:27,578:INFO:Initializing K Neighbors Classifier
2025-10-28 00:09:27,578:INFO:Total runtime is 0.04296300411224366 minutes
2025-10-28 00:09:27,580:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:27,580:INFO:Initializing create_model()
2025-10-28 00:09:27,580:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:27,580:INFO:Checking exceptions
2025-10-28 00:09:27,580:INFO:Importing libraries
2025-10-28 00:09:27,580:INFO:Copying training dataset
2025-10-28 00:09:27,582:INFO:Defining folds
2025-10-28 00:09:27,582:INFO:Declaring metric variables
2025-10-28 00:09:27,584:INFO:Importing untrained model
2025-10-28 00:09:27,587:INFO:K Neighbors Classifier Imported successfully
2025-10-28 00:09:27,591:INFO:Starting cross validation
2025-10-28 00:09:27,591:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:29,256:INFO:Calculating mean and std
2025-10-28 00:09:29,257:INFO:Creating metrics dataframe
2025-10-28 00:09:29,258:INFO:Uploading results into container
2025-10-28 00:09:29,258:INFO:Uploading model into container now
2025-10-28 00:09:29,258:INFO:_master_model_container: 2
2025-10-28 00:09:29,259:INFO:_display_container: 2
2025-10-28 00:09:29,259:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-10-28 00:09:29,259:INFO:create_model() successfully completed......................................
2025-10-28 00:09:29,374:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:29,374:INFO:Creating metrics dataframe
2025-10-28 00:09:29,378:INFO:Initializing Naive Bayes
2025-10-28 00:09:29,378:INFO:Total runtime is 0.07295801639556886 minutes
2025-10-28 00:09:29,379:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:29,379:INFO:Initializing create_model()
2025-10-28 00:09:29,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:29,379:INFO:Checking exceptions
2025-10-28 00:09:29,379:INFO:Importing libraries
2025-10-28 00:09:29,380:INFO:Copying training dataset
2025-10-28 00:09:29,382:INFO:Defining folds
2025-10-28 00:09:29,382:INFO:Declaring metric variables
2025-10-28 00:09:29,384:INFO:Importing untrained model
2025-10-28 00:09:29,387:INFO:Naive Bayes Imported successfully
2025-10-28 00:09:29,390:INFO:Starting cross validation
2025-10-28 00:09:29,391:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:29,449:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,450:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,454:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,456:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,482:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,491:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,505:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:29,519:INFO:Calculating mean and std
2025-10-28 00:09:29,520:INFO:Creating metrics dataframe
2025-10-28 00:09:29,522:INFO:Uploading results into container
2025-10-28 00:09:29,522:INFO:Uploading model into container now
2025-10-28 00:09:29,523:INFO:_master_model_container: 3
2025-10-28 00:09:29,523:INFO:_display_container: 2
2025-10-28 00:09:29,523:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-10-28 00:09:29,523:INFO:create_model() successfully completed......................................
2025-10-28 00:09:29,630:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:29,630:INFO:Creating metrics dataframe
2025-10-28 00:09:29,633:INFO:Initializing Decision Tree Classifier
2025-10-28 00:09:29,634:INFO:Total runtime is 0.0772204359372457 minutes
2025-10-28 00:09:29,635:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:29,636:INFO:Initializing create_model()
2025-10-28 00:09:29,636:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:29,636:INFO:Checking exceptions
2025-10-28 00:09:29,636:INFO:Importing libraries
2025-10-28 00:09:29,636:INFO:Copying training dataset
2025-10-28 00:09:29,640:INFO:Defining folds
2025-10-28 00:09:29,640:INFO:Declaring metric variables
2025-10-28 00:09:29,642:INFO:Importing untrained model
2025-10-28 00:09:29,644:INFO:Decision Tree Classifier Imported successfully
2025-10-28 00:09:29,647:INFO:Starting cross validation
2025-10-28 00:09:29,647:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:29,776:INFO:Calculating mean and std
2025-10-28 00:09:29,777:INFO:Creating metrics dataframe
2025-10-28 00:09:29,779:INFO:Uploading results into container
2025-10-28 00:09:29,780:INFO:Uploading model into container now
2025-10-28 00:09:29,780:INFO:_master_model_container: 4
2025-10-28 00:09:29,780:INFO:_display_container: 2
2025-10-28 00:09:29,780:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-10-28 00:09:29,780:INFO:create_model() successfully completed......................................
2025-10-28 00:09:29,890:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:29,890:INFO:Creating metrics dataframe
2025-10-28 00:09:29,893:INFO:Initializing SVM - Linear Kernel
2025-10-28 00:09:29,893:INFO:Total runtime is 0.08155151208241782 minutes
2025-10-28 00:09:29,895:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:29,895:INFO:Initializing create_model()
2025-10-28 00:09:29,895:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:29,895:INFO:Checking exceptions
2025-10-28 00:09:29,895:INFO:Importing libraries
2025-10-28 00:09:29,895:INFO:Copying training dataset
2025-10-28 00:09:29,898:INFO:Defining folds
2025-10-28 00:09:29,898:INFO:Declaring metric variables
2025-10-28 00:09:29,900:INFO:Importing untrained model
2025-10-28 00:09:29,902:INFO:SVM - Linear Kernel Imported successfully
2025-10-28 00:09:29,906:INFO:Starting cross validation
2025-10-28 00:09:29,907:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:29,984:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:29,988:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:29,996:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,023:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,035:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,042:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,045:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,051:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,057:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,074:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,086:INFO:Calculating mean and std
2025-10-28 00:09:30,087:INFO:Creating metrics dataframe
2025-10-28 00:09:30,090:INFO:Uploading results into container
2025-10-28 00:09:30,091:INFO:Uploading model into container now
2025-10-28 00:09:30,092:INFO:_master_model_container: 5
2025-10-28 00:09:30,092:INFO:_display_container: 2
2025-10-28 00:09:30,093:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-10-28 00:09:30,093:INFO:create_model() successfully completed......................................
2025-10-28 00:09:30,242:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:30,242:INFO:Creating metrics dataframe
2025-10-28 00:09:30,246:INFO:Initializing Ridge Classifier
2025-10-28 00:09:30,246:INFO:Total runtime is 0.08742413520812989 minutes
2025-10-28 00:09:30,247:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:30,248:INFO:Initializing create_model()
2025-10-28 00:09:30,248:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:30,248:INFO:Checking exceptions
2025-10-28 00:09:30,248:INFO:Importing libraries
2025-10-28 00:09:30,248:INFO:Copying training dataset
2025-10-28 00:09:30,250:INFO:Defining folds
2025-10-28 00:09:30,250:INFO:Declaring metric variables
2025-10-28 00:09:30,253:INFO:Importing untrained model
2025-10-28 00:09:30,255:INFO:Ridge Classifier Imported successfully
2025-10-28 00:09:30,259:INFO:Starting cross validation
2025-10-28 00:09:30,260:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:30,327:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,328:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,330:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,330:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,330:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,331:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,333:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,336:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,349:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,351:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,357:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,360:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,363:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,366:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,369:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,370:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,373:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,374:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,379:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:30,382:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:30,388:INFO:Calculating mean and std
2025-10-28 00:09:30,389:INFO:Creating metrics dataframe
2025-10-28 00:09:30,391:INFO:Uploading results into container
2025-10-28 00:09:30,391:INFO:Uploading model into container now
2025-10-28 00:09:30,391:INFO:_master_model_container: 6
2025-10-28 00:09:30,391:INFO:_display_container: 2
2025-10-28 00:09:30,392:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-10-28 00:09:30,392:INFO:create_model() successfully completed......................................
2025-10-28 00:09:30,505:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:30,505:INFO:Creating metrics dataframe
2025-10-28 00:09:30,510:INFO:Initializing Random Forest Classifier
2025-10-28 00:09:30,510:INFO:Total runtime is 0.09182432095209758 minutes
2025-10-28 00:09:30,511:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:30,512:INFO:Initializing create_model()
2025-10-28 00:09:30,512:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:30,512:INFO:Checking exceptions
2025-10-28 00:09:30,512:INFO:Importing libraries
2025-10-28 00:09:30,512:INFO:Copying training dataset
2025-10-28 00:09:30,514:INFO:Defining folds
2025-10-28 00:09:30,514:INFO:Declaring metric variables
2025-10-28 00:09:30,516:INFO:Importing untrained model
2025-10-28 00:09:30,518:INFO:Random Forest Classifier Imported successfully
2025-10-28 00:09:30,522:INFO:Starting cross validation
2025-10-28 00:09:30,523:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:30,970:INFO:Calculating mean and std
2025-10-28 00:09:30,971:INFO:Creating metrics dataframe
2025-10-28 00:09:30,972:INFO:Uploading results into container
2025-10-28 00:09:30,972:INFO:Uploading model into container now
2025-10-28 00:09:30,973:INFO:_master_model_container: 7
2025-10-28 00:09:30,973:INFO:_display_container: 2
2025-10-28 00:09:30,973:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-10-28 00:09:30,973:INFO:create_model() successfully completed......................................
2025-10-28 00:09:31,082:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:31,082:INFO:Creating metrics dataframe
2025-10-28 00:09:31,087:INFO:Initializing Quadratic Discriminant Analysis
2025-10-28 00:09:31,087:INFO:Total runtime is 0.10144134759902955 minutes
2025-10-28 00:09:31,089:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:31,089:INFO:Initializing create_model()
2025-10-28 00:09:31,089:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:31,089:INFO:Checking exceptions
2025-10-28 00:09:31,089:INFO:Importing libraries
2025-10-28 00:09:31,089:INFO:Copying training dataset
2025-10-28 00:09:31,092:INFO:Defining folds
2025-10-28 00:09:31,092:INFO:Declaring metric variables
2025-10-28 00:09:31,094:INFO:Importing untrained model
2025-10-28 00:09:31,096:INFO:Quadratic Discriminant Analysis Imported successfully
2025-10-28 00:09:31,101:INFO:Starting cross validation
2025-10-28 00:09:31,103:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:31,188:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,201:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,203:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,219:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,225:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,226:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,233:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,237:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,240:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,241:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,262:INFO:Calculating mean and std
2025-10-28 00:09:31,262:INFO:Creating metrics dataframe
2025-10-28 00:09:31,264:INFO:Uploading results into container
2025-10-28 00:09:31,264:INFO:Uploading model into container now
2025-10-28 00:09:31,264:INFO:_master_model_container: 8
2025-10-28 00:09:31,264:INFO:_display_container: 2
2025-10-28 00:09:31,265:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-10-28 00:09:31,265:INFO:create_model() successfully completed......................................
2025-10-28 00:09:31,373:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:31,374:INFO:Creating metrics dataframe
2025-10-28 00:09:31,379:INFO:Initializing Ada Boost Classifier
2025-10-28 00:09:31,379:INFO:Total runtime is 0.10630567073822023 minutes
2025-10-28 00:09:31,381:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:31,381:INFO:Initializing create_model()
2025-10-28 00:09:31,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:31,381:INFO:Checking exceptions
2025-10-28 00:09:31,381:INFO:Importing libraries
2025-10-28 00:09:31,381:INFO:Copying training dataset
2025-10-28 00:09:31,384:INFO:Defining folds
2025-10-28 00:09:31,384:INFO:Declaring metric variables
2025-10-28 00:09:31,386:INFO:Importing untrained model
2025-10-28 00:09:31,388:INFO:Ada Boost Classifier Imported successfully
2025-10-28 00:09:31,391:INFO:Starting cross validation
2025-10-28 00:09:31,392:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:31,432:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,433:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,434:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,440:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,479:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,483:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,484:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,487:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,490:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,491:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-10-28 00:09:31,531:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,533:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,543:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,547:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,597:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,610:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,617:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,626:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,633:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,638:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:31,650:INFO:Calculating mean and std
2025-10-28 00:09:31,651:INFO:Creating metrics dataframe
2025-10-28 00:09:31,653:INFO:Uploading results into container
2025-10-28 00:09:31,654:INFO:Uploading model into container now
2025-10-28 00:09:31,654:INFO:_master_model_container: 9
2025-10-28 00:09:31,654:INFO:_display_container: 2
2025-10-28 00:09:31,654:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-10-28 00:09:31,655:INFO:create_model() successfully completed......................................
2025-10-28 00:09:31,763:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:31,763:INFO:Creating metrics dataframe
2025-10-28 00:09:31,769:INFO:Initializing Gradient Boosting Classifier
2025-10-28 00:09:31,769:INFO:Total runtime is 0.11281344095865886 minutes
2025-10-28 00:09:31,771:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:31,771:INFO:Initializing create_model()
2025-10-28 00:09:31,771:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:31,772:INFO:Checking exceptions
2025-10-28 00:09:31,772:INFO:Importing libraries
2025-10-28 00:09:31,772:INFO:Copying training dataset
2025-10-28 00:09:31,775:INFO:Defining folds
2025-10-28 00:09:31,775:INFO:Declaring metric variables
2025-10-28 00:09:31,777:INFO:Importing untrained model
2025-10-28 00:09:31,779:INFO:Gradient Boosting Classifier Imported successfully
2025-10-28 00:09:31,783:INFO:Starting cross validation
2025-10-28 00:09:31,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:32,978:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:32,989:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,059:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,071:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,296:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,381:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,416:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,449:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,552:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,574:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,587:INFO:Calculating mean and std
2025-10-28 00:09:33,589:INFO:Creating metrics dataframe
2025-10-28 00:09:33,590:INFO:Uploading results into container
2025-10-28 00:09:33,591:INFO:Uploading model into container now
2025-10-28 00:09:33,591:INFO:_master_model_container: 10
2025-10-28 00:09:33,591:INFO:_display_container: 2
2025-10-28 00:09:33,591:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-28 00:09:33,591:INFO:create_model() successfully completed......................................
2025-10-28 00:09:33,719:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:33,719:INFO:Creating metrics dataframe
2025-10-28 00:09:33,725:INFO:Initializing Linear Discriminant Analysis
2025-10-28 00:09:33,725:INFO:Total runtime is 0.14540710449218752 minutes
2025-10-28 00:09:33,727:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:33,727:INFO:Initializing create_model()
2025-10-28 00:09:33,727:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:33,727:INFO:Checking exceptions
2025-10-28 00:09:33,727:INFO:Importing libraries
2025-10-28 00:09:33,727:INFO:Copying training dataset
2025-10-28 00:09:33,730:INFO:Defining folds
2025-10-28 00:09:33,730:INFO:Declaring metric variables
2025-10-28 00:09:33,731:INFO:Importing untrained model
2025-10-28 00:09:33,734:INFO:Linear Discriminant Analysis Imported successfully
2025-10-28 00:09:33,738:INFO:Starting cross validation
2025-10-28 00:09:33,739:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:33,813:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,814:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,816:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,844:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,850:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,855:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:33,863:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,864:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,868:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:33,869:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,870:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,884:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-10-28 00:09:33,899:INFO:Calculating mean and std
2025-10-28 00:09:33,901:INFO:Creating metrics dataframe
2025-10-28 00:09:33,904:INFO:Uploading results into container
2025-10-28 00:09:33,904:INFO:Uploading model into container now
2025-10-28 00:09:33,905:INFO:_master_model_container: 11
2025-10-28 00:09:33,905:INFO:_display_container: 2
2025-10-28 00:09:33,905:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-10-28 00:09:33,905:INFO:create_model() successfully completed......................................
2025-10-28 00:09:34,040:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:34,040:INFO:Creating metrics dataframe
2025-10-28 00:09:34,045:INFO:Initializing Extra Trees Classifier
2025-10-28 00:09:34,045:INFO:Total runtime is 0.15074920256932578 minutes
2025-10-28 00:09:34,048:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:34,048:INFO:Initializing create_model()
2025-10-28 00:09:34,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:34,048:INFO:Checking exceptions
2025-10-28 00:09:34,048:INFO:Importing libraries
2025-10-28 00:09:34,048:INFO:Copying training dataset
2025-10-28 00:09:34,053:INFO:Defining folds
2025-10-28 00:09:34,053:INFO:Declaring metric variables
2025-10-28 00:09:34,055:INFO:Importing untrained model
2025-10-28 00:09:34,058:INFO:Extra Trees Classifier Imported successfully
2025-10-28 00:09:34,062:INFO:Starting cross validation
2025-10-28 00:09:34,063:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:34,564:INFO:Calculating mean and std
2025-10-28 00:09:34,565:INFO:Creating metrics dataframe
2025-10-28 00:09:34,567:INFO:Uploading results into container
2025-10-28 00:09:34,567:INFO:Uploading model into container now
2025-10-28 00:09:34,567:INFO:_master_model_container: 12
2025-10-28 00:09:34,567:INFO:_display_container: 2
2025-10-28 00:09:34,568:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-10-28 00:09:34,568:INFO:create_model() successfully completed......................................
2025-10-28 00:09:34,715:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:34,715:INFO:Creating metrics dataframe
2025-10-28 00:09:34,721:INFO:Initializing Light Gradient Boosting Machine
2025-10-28 00:09:34,721:INFO:Total runtime is 0.16201515992482504 minutes
2025-10-28 00:09:34,723:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:34,724:INFO:Initializing create_model()
2025-10-28 00:09:34,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:34,724:INFO:Checking exceptions
2025-10-28 00:09:34,724:INFO:Importing libraries
2025-10-28 00:09:34,724:INFO:Copying training dataset
2025-10-28 00:09:34,727:INFO:Defining folds
2025-10-28 00:09:34,727:INFO:Declaring metric variables
2025-10-28 00:09:34,729:INFO:Importing untrained model
2025-10-28 00:09:34,731:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-28 00:09:34,735:INFO:Starting cross validation
2025-10-28 00:09:34,735:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:44,993:WARNING:Exception in thread Thread-48:
2025-10-28 00:09:44,993:WARNING:Traceback (most recent call last):
2025-10-28 00:09:44,993:WARNING:  File "/usr/lib/python3.11/threading.py", line 1045, in _bootstrap_inner
2025-10-28 00:09:44,994:WARNING:    self.run()
2025-10-28 00:09:44,994:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/_monitor.py", line 84, in run
2025-10-28 00:09:44,994:WARNING:    instance.refresh(nolock=True)
2025-10-28 00:09:44,994:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/std.py", line 1347, in refresh
2025-10-28 00:09:44,996:WARNING:    self.display()
2025-10-28 00:09:44,996:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/display/progress_bar.py", line 71, in display
2025-10-28 00:09:44,996:WARNING:    super().display(msg, pos, close, bar_style, check_delay)
2025-10-28 00:09:44,996:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/tqdm/notebook.py", line 157, in display
2025-10-28 00:09:44,997:WARNING:    pbar.value = self.n
2025-10-28 00:09:44,997:WARNING:    ^^^^^^^^^^
2025-10-28 00:09:44,997:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 716, in __set__
2025-10-28 00:09:44,997:WARNING:    self.set(obj, value)
2025-10-28 00:09:44,997:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 706, in set
2025-10-28 00:09:44,997:WARNING:    obj._notify_trait(self.name, old_value, new_value)
2025-10-28 00:09:44,997:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/traitlets/traitlets.py", line 1513, in _notify_trait
2025-10-28 00:09:44,998:WARNING:    self.notify_change(
2025-10-28 00:09:44,998:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 700, in notify_change
2025-10-28 00:09:44,998:WARNING:    self.send_state(key=name)
2025-10-28 00:09:44,998:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 586, in send_state
2025-10-28 00:09:44,998:WARNING:    self._send(msg, buffers=buffers)
2025-10-28 00:09:44,998:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipywidgets/widgets/widget.py", line 825, in _send
2025-10-28 00:09:44,999:WARNING:    self.comm.send(data=msg, buffers=buffers)
2025-10-28 00:09:44,999:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/comm/base_comm.py", line 144, in send
2025-10-28 00:09:44,999:WARNING:    self.publish_msg(
2025-10-28 00:09:44,999:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipykernel/comm/comm.py", line 42, in publish_msg
2025-10-28 00:09:45,000:WARNING:    parent=self.kernel.get_parent(),
2025-10-28 00:09:45,000:WARNING:           ^^^^^^^^^^^^^^^^^^^^^^^^
2025-10-28 00:09:45,000:WARNING:  File "/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py", line 797, in get_parent
2025-10-28 00:09:45,000:WARNING:    return self._shell_parent.get()
2025-10-28 00:09:45,000:WARNING:           ^^^^^^^^^^^^^^^^^^^^^^^^
2025-10-28 00:09:45,000:WARNING:LookupError: <ContextVar name='shell_parent' at 0x7f62256dd0d0>
2025-10-28 00:09:47,976:INFO:Calculating mean and std
2025-10-28 00:09:47,977:INFO:Creating metrics dataframe
2025-10-28 00:09:47,979:INFO:Uploading results into container
2025-10-28 00:09:47,979:INFO:Uploading model into container now
2025-10-28 00:09:47,979:INFO:_master_model_container: 13
2025-10-28 00:09:47,980:INFO:_display_container: 2
2025-10-28 00:09:47,980:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-28 00:09:47,980:INFO:create_model() successfully completed......................................
2025-10-28 00:09:48,087:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:48,087:INFO:Creating metrics dataframe
2025-10-28 00:09:48,092:INFO:Initializing Dummy Classifier
2025-10-28 00:09:48,092:INFO:Total runtime is 0.38485504388809205 minutes
2025-10-28 00:09:48,093:INFO:SubProcess create_model() called ==================================
2025-10-28 00:09:48,093:INFO:Initializing create_model()
2025-10-28 00:09:48,093:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f6190418190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:48,093:INFO:Checking exceptions
2025-10-28 00:09:48,093:INFO:Importing libraries
2025-10-28 00:09:48,093:INFO:Copying training dataset
2025-10-28 00:09:48,096:INFO:Defining folds
2025-10-28 00:09:48,096:INFO:Declaring metric variables
2025-10-28 00:09:48,098:INFO:Importing untrained model
2025-10-28 00:09:48,100:INFO:Dummy Classifier Imported successfully
2025-10-28 00:09:48,103:INFO:Starting cross validation
2025-10-28 00:09:48,103:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:09:48,179:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,194:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,198:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,208:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,209:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,220:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,229:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,232:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,235:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,235:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-10-28 00:09:48,243:INFO:Calculating mean and std
2025-10-28 00:09:48,244:INFO:Creating metrics dataframe
2025-10-28 00:09:48,245:INFO:Uploading results into container
2025-10-28 00:09:48,246:INFO:Uploading model into container now
2025-10-28 00:09:48,246:INFO:_master_model_container: 14
2025-10-28 00:09:48,246:INFO:_display_container: 2
2025-10-28 00:09:48,246:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-10-28 00:09:48,246:INFO:create_model() successfully completed......................................
2025-10-28 00:09:48,353:INFO:SubProcess create_model() end ==================================
2025-10-28 00:09:48,353:INFO:Creating metrics dataframe
2025-10-28 00:09:48,359:WARNING:/home/ykalathiya/pycaret_assignment/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2025-10-28 00:09:48,363:INFO:Initializing create_model()
2025-10-28 00:09:48,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:48,363:INFO:Checking exceptions
2025-10-28 00:09:48,364:INFO:Importing libraries
2025-10-28 00:09:48,364:INFO:Copying training dataset
2025-10-28 00:09:48,367:INFO:Defining folds
2025-10-28 00:09:48,367:INFO:Declaring metric variables
2025-10-28 00:09:48,367:INFO:Importing untrained model
2025-10-28 00:09:48,367:INFO:Declaring custom model
2025-10-28 00:09:48,367:INFO:Gradient Boosting Classifier Imported successfully
2025-10-28 00:09:48,368:INFO:Cross validation set to False
2025-10-28 00:09:48,368:INFO:Fitting Model
2025-10-28 00:09:49,384:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-28 00:09:49,384:INFO:create_model() successfully completed......................................
2025-10-28 00:09:49,505:INFO:_master_model_container: 14
2025-10-28 00:09:49,505:INFO:_display_container: 2
2025-10-28 00:09:49,506:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-10-28 00:09:49,506:INFO:compare_models() successfully completed......................................
2025-10-28 00:09:49,516:INFO:Initializing evaluate_model()
2025-10-28 00:09:49,516:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2025-10-28 00:09:49,521:INFO:Initializing plot_model()
2025-10-28 00:09:49,521:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-10-28 00:09:49,521:INFO:Checking exceptions
2025-10-28 00:09:49,524:INFO:Preloading libraries
2025-10-28 00:09:49,537:INFO:Copying training dataset
2025-10-28 00:09:49,537:INFO:Plot type: pipeline
2025-10-28 00:09:49,649:INFO:Visual Rendered Successfully
2025-10-28 00:09:49,789:INFO:plot_model() successfully completed......................................
2025-10-28 00:09:49,804:INFO:Initializing save_model()
2025-10-28 00:09:49,804:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=best_student_performance_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=Non...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-10-28 00:09:49,804:INFO:Adding model into prep_pipe
2025-10-28 00:09:49,818:INFO:best_student_performance_model.pkl saved in current working directory
2025-10-28 00:09:49,822:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empt...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=123, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-10-28 00:09:49,822:INFO:save_model() successfully completed......................................
2025-10-28 00:09:49,948:INFO:Initializing predict_model()
2025-10-28 00:09:49,948:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f61906ab560>)
2025-10-28 00:09:49,948:INFO:Checking exceptions
2025-10-28 00:09:49,949:INFO:Preloading libraries
2025-10-28 00:09:50,209:INFO:Initializing create_model()
2025-10-28 00:09:50,209:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-10-28 00:09:50,209:INFO:Checking exceptions
2025-10-28 00:09:50,217:INFO:Importing libraries
2025-10-28 00:09:50,217:INFO:Copying training dataset
2025-10-28 00:09:50,222:INFO:Defining folds
2025-10-28 00:09:50,222:INFO:Declaring metric variables
2025-10-28 00:09:50,225:INFO:Importing untrained model
2025-10-28 00:09:50,231:INFO:Light Gradient Boosting Machine Imported successfully
2025-10-28 00:09:50,244:INFO:Starting cross validation
2025-10-28 00:09:50,246:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-10-28 00:10:05,844:INFO:Calculating mean and std
2025-10-28 00:10:05,846:INFO:Creating metrics dataframe
2025-10-28 00:10:05,850:INFO:Finalizing model
2025-10-28 00:10:05,898:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.
2025-10-28 00:10:05,898:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-10-28 00:10:05,898:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Total Bins 581
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Number of data points in the train set: 1674, number of used features: 13
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Start training from score -3.105483
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Start training from score -2.186529
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Start training from score -1.809843
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Start training from score -1.753090
2025-10-28 00:10:05,899:INFO:[LightGBM] [Info] Start training from score -0.681271
2025-10-28 00:10:05,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-28 00:10:05,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-10-28 00:10:08,047:INFO:Uploading results into container
2025-10-28 00:10:08,048:INFO:Uploading model into container now
2025-10-28 00:10:08,056:INFO:_master_model_container: 15
2025-10-28 00:10:08,056:INFO:_display_container: 4
2025-10-28 00:10:08,057:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-10-28 00:10:08,057:INFO:create_model() successfully completed......................................
2025-10-28 00:10:08,245:INFO:Soft dependency imported: gradio: 5.49.1
2025-10-28 00:11:33,308:INFO:Initializing plot_model()
2025-10-28 00:11:33,309:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2025-10-28 00:11:33,309:INFO:Checking exceptions
2025-10-28 00:11:33,312:INFO:Preloading libraries
2025-10-28 00:11:33,325:INFO:Copying training dataset
2025-10-28 00:11:33,325:INFO:Plot type: feature_all
2025-10-28 00:11:33,341:WARNING:No coef_ found. Trying feature_importances_
2025-10-28 00:11:33,523:INFO:Visual Rendered Successfully
2025-10-28 00:11:33,664:INFO:plot_model() successfully completed......................................
2025-10-28 00:13:39,352:INFO:Initializing predict_model()
2025-10-28 00:13:39,352:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6224c5a2d0>, estimator=LGBMClassifier(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=True, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f619056dbc0>)
2025-10-28 00:13:39,352:INFO:Checking exceptions
2025-10-28 00:13:39,352:INFO:Preloading libraries
2025-10-28 00:13:39,354:INFO:Set up data.
2025-10-28 00:13:39,359:INFO:Set up index.
